{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 1292 unique combinations\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4689d350d5a4429886fc048dc4a83eb8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=4.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3c1e26c47c9d489b9f1a68e31dc87ca7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50210.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fcb32e7aeeee4d458045f1d12d440298",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50210.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "098f4a173b5a44278e831b27b29c29ec",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50210.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e970e78f9b3441bb2822adcd5cef33b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=50210.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Done wrote to images\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "import argparse\n",
    "import os\n",
    "\n",
    "\n",
    "def normalize_image(img, org_width, org_height, new_width, new_height):\n",
    "  # Invert\n",
    "  img = 255 - img\n",
    "  # Normalize\n",
    "  img = (img * (255.0 / img.max())).astype(np.uint8)\n",
    "  # Reshape\n",
    "  img = img.reshape(org_height, org_width)\n",
    "  image_resized = cv2.resize(img, (new_width, new_height), interpolation=cv2.INTER_LINEAR)\n",
    "  return image_resized\n",
    "\n",
    "\n",
    "def dump_images(args, org_width, org_height, new_width, new_height):\n",
    "  labels = pd.read_csv(args.labels)\n",
    "  iids = labels['image_id']\n",
    "  root = labels['grapheme_root']\n",
    "  vowel = labels['vowel_diacritic']\n",
    "  consonant = labels['consonant_diacritic']\n",
    "  labels = {a: (b, c, d) for a, b, c, d in zip(iids, root, vowel, consonant)}\n",
    "  tuples = sorted(set(labels.values()))\n",
    "  tuples_to_int = {v: k for k, v in enumerate(tuples)}\n",
    "  print(f'Got {len(tuples)} unique combinations')\n",
    "  for i in tqdm(range(0, 4)):\n",
    "    df = pd.read_parquet(args.data_template % i)\n",
    "    image_ids = df['image_id'].values\n",
    "    df = df.drop(['image_id'], axis=1)\n",
    "    for image_id, index in tqdm(zip(image_ids, range(df.shape[0])), total=df.shape[0]):\n",
    "      normalized = normalize_image(df.loc[df.index[index]].values,\n",
    "          org_width, org_height, new_width, new_height)\n",
    "      r, v, c = labels[image_id]\n",
    "      tuple_int = tuples_to_int[(r, v, c)]\n",
    "      # e.g: 'Train_300_rt_29_vl_5_ct_0_ti_179.png'\n",
    "      out_fn = os.path.join(args.image_dir, f'{image_id}_rt_{r}_vl_{v}_ct_{c}_ti_{tuple_int}.png')\n",
    "      cv2.imwrite(out_fn, normalized)\n",
    "\n",
    "\n",
    "def main():\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument('--image_dir', type=str, default='images')\n",
    "  parser.add_argument('--data_template', type=str, default='./data/train_image_data_%d.parquet')\n",
    "  parser.add_argument('--labels', type=str, default='./data/train.csv')\n",
    "  args, _ = parser.parse_known_args()\n",
    "\n",
    "  os.makedirs(args.image_dir, exist_ok=True)\n",
    "\n",
    "  org_height = 137\n",
    "  org_width = 236\n",
    "  new_height = 160  # 5 * 32\n",
    "  new_width = 256  # 8 * 32\n",
    "  dump_images(args, org_width, org_height, new_width, new_height)\n",
    "  print(f'Done wrote to {args.image_dir}')\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160672 training and 40168 validation fns\n",
      "Removing images with bad shape\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bf7f9ea7a0404cd8b1d658739d209a1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=40168.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing TFRecords\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e476e6e59745a79b8121399bc3aa8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "df4dd730178e46d09cdbb7296ab73f42",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20084.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote file records/val_0000_020084_v0.1.0.tfrec containing 20084 records\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a0d9e319cd041528d84471ac1a6b99f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=20084.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote file records/val_0001_020084_v0.1.0.tfrec containing 20084 records\n",
      "\n",
      "Removing images with bad shape\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2e2c9a6e22fe40638d2ba8e405ad867d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=160672.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Writing TFRecords\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd2139e9487c4690aecf13234f4cf526",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=10.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f142be48962348499a9cc983bf7af402",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16067.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote file records/train_0000_016067_v0.1.0.tfrec containing 16067 records\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12fe6d4006fd4cb1bcffa3e17868e3a9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16067.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote file records/train_0001_016067_v0.1.0.tfrec containing 16067 records\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3aa2f9dd51df4c3a890e88d816647ef2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16067.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote file records/train_0002_016067_v0.1.0.tfrec containing 16067 records\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "49f71baccb0943cd88d82c879ccddb9e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16067.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote file records/train_0003_016067_v0.1.0.tfrec containing 16067 records\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc65a937279b4001bb9387351909c6cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16067.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote file records/train_0004_016067_v0.1.0.tfrec containing 16067 records\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c3ae9936516449299033e06e246559d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16067.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote file records/train_0005_016067_v0.1.0.tfrec containing 16067 records\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9d8c7495ab2d419b889a54135e55591a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16067.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote file records/train_0006_016067_v0.1.0.tfrec containing 16067 records\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f862fb1e5c4b48a19112e6a24092d0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16067.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote file records/train_0007_016067_v0.1.0.tfrec containing 16067 records\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "045a11fde3eb4d8486b26a6125facd43",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16067.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote file records/train_0008_016067_v0.1.0.tfrec containing 16067 records\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "896b75c935c74438ba6cbfadcbaa8780",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=16067.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote file records/train_0009_016067_v0.1.0.tfrec containing 16067 records\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "73d28c72319e43c2a1f688aa025bfb07",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Wrote file records/train_0010_000002_v0.1.0.tfrec containing 2 records\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Copyright 2020 Google LLC\n",
    "\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "\n",
    "# http://www.apache.org/licenses/LICENSE-2.0\n",
    "\n",
    "# Unless required by applicable law or agreed to in writing, software distributed\n",
    "# under the License is distributed on an \"AS IS\" BASIS, WITHOUT WARRANTIES OR\n",
    "# CONDITIONS OF ANY KIND, either express or implied. See the License for the\n",
    "# specific language governing permissions and limitations under the License.\n",
    "\n",
    "\"\"\"\n",
    "# author: Martin Gorner\n",
    "# twitter: @martin_gorner\n",
    "# modified: See--\n",
    "# modified from:\n",
    "# https://github.com/GoogleCloudPlatform/training-data-analyst/blob/master/courses/fast-and-lean-data-science/03_Flower_pictures_to_TFRecords.ipynb\n",
    "\"\"\"\n",
    "import tensorflow as tf\n",
    "import os\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tqdm.auto import tqdm\n",
    "import argparse\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "\n",
    "\n",
    "def read_image_label(inputs):\n",
    "  img_bytes = tf.io.read_file(inputs['img'])\n",
    "  return img_bytes, inputs['image_id'], inputs['grapheme_root'], inputs['vowel_diacritic'], \\\n",
    "      inputs['consonant_diacritic'], inputs['unique_tuple']\n",
    "\n",
    "\n",
    "def to_tfrecord(img_bytes, image_id, grapheme_root, vowel_diacritic,\n",
    "      consonant_diacritic, unique_tuple):\n",
    "  feature = {\n",
    "      'img': tf.train.Feature(bytes_list=tf.train.BytesList(value=[img_bytes])),\n",
    "      'image_id': tf.train.Feature(int64_list=tf.train.Int64List(value=[image_id])),\n",
    "      'grapheme_root': tf.train.Feature(int64_list=tf.train.Int64List(value=[grapheme_root])),\n",
    "      'vowel_diacritic': tf.train.Feature(int64_list=tf.train.Int64List(value=[vowel_diacritic])),\n",
    "      'consonant_diacritic': tf.train.Feature(int64_list=tf.train.Int64List(value=[\n",
    "          consonant_diacritic])),\n",
    "      'unique_tuple': tf.train.Feature(int64_list=tf.train.Int64List(value=[unique_tuple])),\n",
    "  }\n",
    "  return tf.train.Example(features=tf.train.Features(feature=feature))\n",
    "\n",
    "\n",
    "def get_img_size(fn):\n",
    "  try:\n",
    "    # width, height = im.size\n",
    "    img_size = Image.open(fn).size[::-1]\n",
    "\n",
    "  except Exception as e:\n",
    "    print(f'{fn} errored with {e}')\n",
    "    img_size = None\n",
    "  return img_size\n",
    "\n",
    "\n",
    "def main():\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument('--clean', action='store_true')\n",
    "  parser.add_argument('--version', type=str, default='v0.1.0')\n",
    "  parser.add_argument('--do_not_train', action='store_true')\n",
    "  parser.add_argument('--records_dir', type=str, default='records')\n",
    "  parser.add_argument('--image_glob', type=str, default='images/*.png')\n",
    "  parser.add_argument('--seed', type=int, default=123)\n",
    "  args, _ = parser.parse_known_args()\n",
    "\n",
    "  np.random.seed(args.seed)\n",
    "  os.makedirs(args.records_dir, exist_ok=True)\n",
    "  if args.clean:\n",
    "    os.system(f'rm -f {args.records_dir}/*.tfrec')\n",
    "    print('Done cleaning')\n",
    "    return 0\n",
    "\n",
    "  fns = sorted(tf.io.gfile.glob(args.image_glob),\n",
    "      key=lambda x: int(x.split('_')[1]))\n",
    "  perm = np.random.permutation(len(fns))\n",
    "  num_train = int(0.8 * len(fns))\n",
    "  train_fns = [fns[p] for p in perm[:num_train]]\n",
    "  val_fns = [fns[p] for p in perm[num_train:]]\n",
    "  print(f'{len(train_fns)} training and {len(val_fns)} validation fns')\n",
    "  num_shards = 1\n",
    "  for prefix in ['val', 'train']:\n",
    "    if prefix == 'train' and args.do_not_train:\n",
    "      continue\n",
    "\n",
    "    if prefix == 'train':\n",
    "      img_filenames = train_fns\n",
    "    else:\n",
    "      img_filenames = val_fns\n",
    "\n",
    "    print('Removing images with bad shape')\n",
    "    # remove images with bad shape\n",
    "    with ThreadPoolExecutor() as e:\n",
    "      img_sizes = list(tqdm(e.map(get_img_size, img_filenames), total=len(\n",
    "        img_filenames)))\n",
    "\n",
    "    img_sizes = [tf.constant(sz, tf.int64) for sz in img_sizes]\n",
    "\n",
    "    # e.g: 'images/Train_116991_rt_53_vl_7_ct_4_ti_343.png'\n",
    "    #       000000000000_111111_22_33_44_5_66_7_88_9999999\n",
    "    image_id = [int(fn.split('_')[1]) for fn in img_filenames]\n",
    "    grapheme_root = [int(fn.split('_')[3]) for fn in img_filenames]\n",
    "    vowel_diacritic = [int(fn.split('_')[5]) for fn in img_filenames]\n",
    "    consonant_diacritic = [int(fn.split('_')[7]) for fn in img_filenames]\n",
    "    unique_tuple = [int(fn.split('_')[9][:-4]) for fn in img_filenames]\n",
    "\n",
    "    if prefix == 'train':\n",
    "      num_shards = 10\n",
    "    else:\n",
    "      num_shards = 2\n",
    "\n",
    "    ds = tf.data.Dataset.from_tensor_slices({'img': img_filenames, 'image_id': image_id,\n",
    "        'grapheme_root': grapheme_root, 'vowel_diacritic': vowel_diacritic,\n",
    "        'consonant_diacritic': consonant_diacritic, 'unique_tuple': unique_tuple})\n",
    "    ds = ds.map(read_image_label)\n",
    "    ds = ds.batch(len(img_filenames) // num_shards)\n",
    "    print(\"Writing TFRecords\")\n",
    "    for shard_index, ret in tqdm(enumerate(ds), total=num_shards):\n",
    "      # batch size used as shard size here\n",
    "      img, image_id, r, v, c, ti = map(lambda x: x.numpy(), ret)\n",
    "      current_shard_size = img.shape[0]\n",
    "      # good practice to have the number of records in the filename\n",
    "      filename = os.path.join(args.records_dir, '%s_%04d_%06d_%s.tfrec' % (\n",
    "          prefix, shard_index, current_shard_size, args.version))\n",
    "      with tf.io.TFRecordWriter(filename) as out_file:\n",
    "        for i in tqdm(range(current_shard_size)):\n",
    "          example = to_tfrecord(img[i], image_id[i], r[i], v[i], c[i], ti[i])\n",
    "          out_file.write(example.SerializeToString())\n",
    "        print(\"Wrote file {} containing {} records\".format(filename, current_shard_size))\n",
    "\n",
    "\n",
    "main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
