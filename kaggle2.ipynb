{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm.notebook import tqdm\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from keras.models import Sequential, Model, load_model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization, GlobalAveragePooling2D, PReLU, GlobalMaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical, Sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.applications.resnet_v2 import ResNet152V2, preprocess_input\n",
    "from keras.applications.xception import Xception\n",
    "from keras.applications.densenet import DenseNet121, DenseNet169\n",
    "import tensorflow as tf\n",
    "from skimage.transform import AffineTransform, warp\n",
    "\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "import sklearn.metrics\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "from efficientnet.keras import EfficientNetB5\n",
    "from keras_squeeze_excite_network.se_resnext import SEResNextImageNet as SEResNext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv(\"data/train.csv\")\n",
    "yEval = pd.read_csv(\"data/test.csv\")\n",
    "classMap = pd.read_csv(\"data/class_map.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"grapheme_root\",\"vowel_diacritic\",\"consonant_diacritic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = y.set_index(\"image_id\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [pq.read_table('data/train_image_data_{0}.parquet'.format(i)) for i in range(4)]\n",
    "tables = [table.to_pandas() for table in tables]\n",
    "df = pd.concat(tables)\n",
    "df = df.set_index(\"image_id\")\n",
    "del tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def affine_image(img):\n",
    "    \"\"\"\n",
    "\n",
    "    Args:\n",
    "        img: (h, w) or (1, h, w)\n",
    "\n",
    "    Returns:\n",
    "        img: (h, w)\n",
    "    \"\"\"\n",
    "    # ch, h, w = img.shape\n",
    "    # img = img / 255.\n",
    "    if img.ndim == 3:\n",
    "        img = img[0]\n",
    "\n",
    "    # --- scale ---\n",
    "    min_scale = 0.8\n",
    "    max_scale = 1.2\n",
    "    sx = np.random.uniform(min_scale, max_scale)\n",
    "    sy = np.random.uniform(min_scale, max_scale)\n",
    "\n",
    "    # --- rotation ---\n",
    "    max_rot_angle = 7\n",
    "    rot_angle = np.random.uniform(-max_rot_angle, max_rot_angle) * np.pi / 180.\n",
    "\n",
    "    # --- shear ---\n",
    "    max_shear_angle = 10\n",
    "    shear_angle = np.random.uniform(-max_shear_angle, max_shear_angle) * np.pi / 180.\n",
    "\n",
    "    # --- translation ---\n",
    "    max_translation = 4\n",
    "    tx = np.random.randint(-max_translation, max_translation)\n",
    "    ty = np.random.randint(-max_translation, max_translation)\n",
    "\n",
    "    tform = AffineTransform(scale=(sx, sy), rotation=rot_angle, shear=shear_angle,\n",
    "                            translation=(tx, ty))\n",
    "    transformed_image = warp(img, tform)\n",
    "    assert transformed_image.ndim == 2\n",
    "    return (transformed_image*255).astype(np.uint8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=(120, 120)\n",
    "\n",
    "def calcRotate(img):\n",
    "    detector = cv2.ORB_create()\n",
    "    keypoints=detector.detect(img)\n",
    "    descriptors=detector.compute(img,keypoints)\n",
    "    angles=[]\n",
    "    weights=[]\n",
    "    for i in descriptors[0]:\n",
    "        if i.angle!=-1:\n",
    "            angles.append(i.angle)\n",
    "            weights.append(i.response)\n",
    "    if len(angles)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.average(angles,weights=weights)\n",
    "\n",
    "def upper(img,mergin=20):\n",
    "    up=0\n",
    "    bottom=img.shape[0]\n",
    "    while bottom-up>2:\n",
    "        mid=(up+bottom)//2\n",
    "        if np.sum(img[up:mid,:])==0:\n",
    "            up=mid\n",
    "        else:\n",
    "            bottom=mid\n",
    "    return max(up-mergin,0)\n",
    "\n",
    "def lower(img,mergin=20):\n",
    "    up=0\n",
    "    bottom=img.shape[0]\n",
    "    while bottom-up>2:\n",
    "        mid=(up+bottom)//2\n",
    "        if np.sum(img[mid:bottom,:])==0:\n",
    "            bottom=mid\n",
    "        else:\n",
    "            up=mid\n",
    "    return min(bottom+mergin,img.shape[0])\n",
    "\n",
    "def lefter(img,mergin=20):\n",
    "    left=0\n",
    "    right=img.shape[1]\n",
    "    while right-left>2:\n",
    "        mid=(left+right)//2\n",
    "        if np.sum(img[:,left:mid])==0:\n",
    "            left=mid\n",
    "        else:\n",
    "            right=mid\n",
    "    return max(left-mergin,0)\n",
    "\n",
    "def righter(img,mergin=20):\n",
    "    left=0\n",
    "    right=img.shape[1]\n",
    "    while right-left>2:\n",
    "        mid=(left+right)//2\n",
    "        if np.sum(img[:,mid:right])==0:\n",
    "            right=mid\n",
    "        else:\n",
    "            left=mid\n",
    "    return min(right+mergin,img.shape[1])\n",
    "\n",
    "def transformImg(img,size=(255,255),training=True):\n",
    "    ret2, img = cv2.threshold(img, 0, 255, cv2.THRESH_OTSU)\n",
    "    img = 255-img\n",
    "    if training:\n",
    "        img=affine_image(img)\n",
    "    img = img[upper(img):lower(img),lefter(img):righter(img)]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    img = cv2.resize(img, (size[0],size[1]))\n",
    "    kernel = np.ones((3,3),np.float32)/9\n",
    "    img = cv2.filter2D(img,-1,kernel)\n",
    "    return img\n",
    "\n",
    "def randomErase(img, prob=True):\n",
    "    # random erasing\n",
    "    # https://github.com/yu4u/cutout-random-erasing\n",
    "    p = 0.5\n",
    "    s_l = 0.02\n",
    "    s_h = 0.4\n",
    "    r_1 = 0.3\n",
    "    r_2 = 1 / 0.3\n",
    "    v_l = 0\n",
    "    v_h = 255\n",
    "    input_size=size[0]\n",
    "    if prob==False or np.random.random()<p:\n",
    "\n",
    "        while True:\n",
    "            s = np.random.uniform(s_l, s_h) * input_size * input_size\n",
    "            r = np.random.uniform(r_1, r_2)\n",
    "            w = int(np.sqrt(s / r))\n",
    "            h = int(np.sqrt(s * r))\n",
    "            left = np.random.randint(0, input_size)\n",
    "            top = np.random.randint(0, input_size)\n",
    "            if left + w <= input_size and top + h <= input_size:\n",
    "                break\n",
    "        c = np.random.uniform(v_l, v_h, (h, w, 3))\n",
    "        img[top : top + h, left : left + w, :] = c\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(Sequence):\n",
    "    def __init__(self,X,y,training,batch_size=64,size=(255,255),alpha=3):\n",
    "        self.training = training\n",
    "        self.batch_size=batch_size\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        self.size=size\n",
    "        self.alpha=alpha\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.X.shape[0] / self.batch_size))\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        _imgs=self.X[idx * self.batch_size:(idx + 1) * self.batch_size,:,:]\n",
    "        \n",
    "            \n",
    "        \n",
    "        imgs=[]\n",
    "        for img in _imgs:\n",
    "            imgs.append(transformImg(img,size=self.size,training=self.training))\n",
    "         \n",
    "        \n",
    "        \n",
    "        imgs=np.asarray(imgs)\n",
    "        \n",
    "        \n",
    "        ret_y=[]\n",
    "        for label in labels:\n",
    "            ret_y.append(to_categorical(self.y[idx * self.batch_size:(idx + 1) * self.batch_size][label],num_classes=len(set(y[label]))))\n",
    "    \n",
    "    \n",
    "        #mix up\n",
    "        if self.training :\n",
    "            r = np.random.permutation(imgs.shape[0])\n",
    "            imgs2=deepcopy(imgs)[r]\n",
    "            grapheme=ret_y[0]\n",
    "            vowel=ret_y[1]\n",
    "            consonant=ret_y[2]\n",
    "            grapheme2=deepcopy(grapheme)[r]\n",
    "            vowel2=deepcopy(vowel)[r]\n",
    "            consonant2=deepcopy(consonant)[r]\n",
    "            ratio=np.random.beta(self.alpha,self.alpha,imgs.shape[0])\n",
    "            ratio[ratio>1]=1\n",
    "            ratio[ratio<0]=0\n",
    "            imgs=np.tile(ratio,(3,*size,1)).T*imgs+np.tile((1-ratio),(3,*size,1)).T*imgs2\n",
    "            grapheme=np.tile(ratio,(168,1)).T*grapheme+np.tile((1-ratio),(168,1)).T*grapheme2\n",
    "            vowel=np.tile(ratio,(11,1)).T*vowel+np.tile((1-ratio),(11,1)).T*vowel2\n",
    "            consonant=np.tile(ratio,(7,1)).T*consonant+np.tile((1-ratio),(7,1)).T*consonant2\n",
    "            grapheme=grapheme.astype(np.float32)\n",
    "            vowel=vowel.astype(np.float32)\n",
    "            consonant=consonant.astype(np.float32)\n",
    "            ret_y=[grapheme,vowel,consonant]\n",
    "   \n",
    "        if self.training:\n",
    "            #imgs = [randomErase(img) for img in imgs]\n",
    "            pass\n",
    "            \n",
    "            \n",
    "        imgs = np.asarray(imgs).astype(np.float32)/255.0\n",
    "            \n",
    "\n",
    "        return imgs, ret_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getMultiBased():\n",
    "    model =  EfficientNetB5(weights=\"imagenet\", include_top=False, input_shape=(*size, 3))\n",
    "    x = model.output  \n",
    "    x = GlobalAveragePooling2D()(x)\n",
    "    x = Dropout(0.5)(x)\n",
    "    #x = Dense(1024, activation=\"relu\")(x)\n",
    "    grapheme = Dense(168, activation=\"softmax\")(x)\n",
    "    vowel = Dense(11,activation=\"softmax\")(x)\n",
    "    consonant = Dense(7, activation=\"softmax\")(x)\n",
    "    model = Model(inputs=model.input, outputs=[grapheme,vowel,consonant])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.values.reshape(-1,137,236), y, train_size=0.9, random_state=8000)\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "early_stopping = EarlyStopping(monitor='val_loss', min_delta=0.0, patience=20)\n",
    "checkpoint = ModelCheckpoint(filepath=\"tmp-eff5-epoch{epoch:04}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compiled\n",
      "Epoch 1/80\n",
      "2825/2825 [==============================] - 1574s 557ms/step - loss: 0.7154 - dense_1_loss: 0.8978 - dense_2_loss: 0.6320 - dense_3_loss: 0.4340 - dense_1_acc: 0.7566 - dense_2_acc: 0.8679 - dense_3_acc: 0.9057 - val_loss: 0.0565 - val_dense_1_loss: 0.1570 - val_dense_2_loss: 0.0548 - val_dense_3_loss: 0.0470 - val_dense_1_acc: 0.9580 - val_dense_2_acc: 0.9865 - val_dense_3_acc: 0.9881\n",
      "Epoch 2/80\n",
      "2825/2825 [==============================] - 1447s 512ms/step - loss: 0.7119 - dense_1_loss: 0.8923 - dense_2_loss: 0.6315 - dense_3_loss: 0.4317 - dense_1_acc: 0.7578 - dense_2_acc: 0.8684 - dense_3_acc: 0.9058 - val_loss: 0.0574 - val_dense_1_loss: 0.1566 - val_dense_2_loss: 0.0570 - val_dense_3_loss: 0.0491 - val_dense_1_acc: 0.9577 - val_dense_2_acc: 0.9852 - val_dense_3_acc: 0.9865\n",
      "Epoch 3/80\n",
      "2825/2825 [==============================] - 1448s 512ms/step - loss: 0.7095 - dense_1_loss: 0.8888 - dense_2_loss: 0.6286 - dense_3_loss: 0.4319 - dense_1_acc: 0.7585 - dense_2_acc: 0.8707 - dense_3_acc: 0.9069 - val_loss: 0.0911 - val_dense_1_loss: 0.1571 - val_dense_2_loss: 0.0515 - val_dense_3_loss: 0.0448 - val_dense_1_acc: 0.9579 - val_dense_2_acc: 0.9873 - val_dense_3_acc: 0.9875\n",
      "Epoch 4/80\n",
      "2825/2825 [==============================] - 1449s 513ms/step - loss: 0.7081 - dense_1_loss: 0.8847 - dense_2_loss: 0.6302 - dense_3_loss: 0.4326 - dense_1_acc: 0.7595 - dense_2_acc: 0.8692 - dense_3_acc: 0.9069 - val_loss: 0.0422 - val_dense_1_loss: 0.1569 - val_dense_2_loss: 0.0518 - val_dense_3_loss: 0.0486 - val_dense_1_acc: 0.9588 - val_dense_2_acc: 0.9875 - val_dense_3_acc: 0.9874\n",
      "Epoch 5/80\n",
      "2825/2825 [==============================] - 1450s 513ms/step - loss: 0.7051 - dense_1_loss: 0.8812 - dense_2_loss: 0.6279 - dense_3_loss: 0.4302 - dense_1_acc: 0.7614 - dense_2_acc: 0.8707 - dense_3_acc: 0.9082 - val_loss: 0.0834 - val_dense_1_loss: 0.1471 - val_dense_2_loss: 0.0528 - val_dense_3_loss: 0.0471 - val_dense_1_acc: 0.9604 - val_dense_2_acc: 0.9867 - val_dense_3_acc: 0.9879\n",
      "Epoch 6/80\n",
      "2825/2825 [==============================] - 1451s 514ms/step - loss: 0.7031 - dense_1_loss: 0.8773 - dense_2_loss: 0.6278 - dense_3_loss: 0.4302 - dense_1_acc: 0.7613 - dense_2_acc: 0.8720 - dense_3_acc: 0.9071 - val_loss: 0.0610 - val_dense_1_loss: 0.1719 - val_dense_2_loss: 0.0597 - val_dense_3_loss: 0.0512 - val_dense_1_acc: 0.9569 - val_dense_2_acc: 0.9863 - val_dense_3_acc: 0.9870\n",
      "Epoch 7/80\n",
      "2825/2825 [==============================] - 1453s 514ms/step - loss: 0.6995 - dense_1_loss: 0.8718 - dense_2_loss: 0.6260 - dense_3_loss: 0.4283 - dense_1_acc: 0.7630 - dense_2_acc: 0.8726 - dense_3_acc: 0.9079 - val_loss: 0.0495 - val_dense_1_loss: 0.1635 - val_dense_2_loss: 0.0686 - val_dense_3_loss: 0.0466 - val_dense_1_acc: 0.9583 - val_dense_2_acc: 0.9821 - val_dense_3_acc: 0.9883\n",
      "Epoch 8/80\n",
      "2825/2825 [==============================] - 1450s 513ms/step - loss: 0.6988 - dense_1_loss: 0.8714 - dense_2_loss: 0.6243 - dense_3_loss: 0.4282 - dense_1_acc: 0.7632 - dense_2_acc: 0.8736 - dense_3_acc: 0.9089 - val_loss: 0.0541 - val_dense_1_loss: 0.1496 - val_dense_2_loss: 0.0514 - val_dense_3_loss: 0.0434 - val_dense_1_acc: 0.9608 - val_dense_2_acc: 0.9879 - val_dense_3_acc: 0.9889\n",
      "Epoch 9/80\n",
      " 460/2825 [===>..........................] - ETA: 19:16 - loss: 0.6964 - dense_1_loss: 0.8645 - dense_2_loss: 0.6289 - dense_3_loss: 0.4278 - dense_1_acc: 0.7560 - dense_2_acc: 0.8709 - dense_3_acc: 0.9066"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2825/2825 [==============================] - 1449s 513ms/step - loss: 0.6919 - dense_1_loss: 0.8608 - dense_2_loss: 0.6206 - dense_3_loss: 0.4253 - dense_1_acc: 0.7667 - dense_2_acc: 0.8753 - dense_3_acc: 0.9106 - val_loss: 0.0648 - val_dense_1_loss: 0.1540 - val_dense_2_loss: 0.0545 - val_dense_3_loss: 0.0470 - val_dense_1_acc: 0.9593 - val_dense_2_acc: 0.9864 - val_dense_3_acc: 0.9879\n",
      "Epoch 13/80\n",
      "2825/2825 [==============================] - 1450s 513ms/step - loss: 0.6893 - dense_1_loss: 0.8567 - dense_2_loss: 0.6212 - dense_3_loss: 0.4228 - dense_1_acc: 0.7674 - dense_2_acc: 0.8750 - dense_3_acc: 0.9116 - val_loss: 0.0608 - val_dense_1_loss: 0.1573 - val_dense_2_loss: 0.0542 - val_dense_3_loss: 0.0508 - val_dense_1_acc: 0.9589 - val_dense_2_acc: 0.9871 - val_dense_3_acc: 0.9877\n",
      "Epoch 14/80\n",
      "2825/2825 [==============================] - 1450s 513ms/step - loss: 0.6876 - dense_1_loss: 0.8527 - dense_2_loss: 0.6213 - dense_3_loss: 0.4237 - dense_1_acc: 0.7692 - dense_2_acc: 0.8773 - dense_3_acc: 0.9109 - val_loss: 0.0763 - val_dense_1_loss: 0.1527 - val_dense_2_loss: 0.0492 - val_dense_3_loss: 0.0457 - val_dense_1_acc: 0.9601 - val_dense_2_acc: 0.9886 - val_dense_3_acc: 0.9877\n",
      "Epoch 15/80\n",
      "2825/2825 [==============================] - 1449s 513ms/step - loss: 0.6880 - dense_1_loss: 0.8532 - dense_2_loss: 0.6220 - dense_3_loss: 0.4237 - dense_1_acc: 0.7694 - dense_2_acc: 0.8762 - dense_3_acc: 0.9092 - val_loss: 0.1817 - val_dense_1_loss: 0.1660 - val_dense_2_loss: 0.0539 - val_dense_3_loss: 0.0499 - val_dense_1_acc: 0.9563 - val_dense_2_acc: 0.9861 - val_dense_3_acc: 0.9881\n",
      "Epoch 16/80\n",
      "1071/2825 [==========>...................] - ETA: 14:19 - loss: 0.6888 - dense_1_loss: 0.8524 - dense_2_loss: 0.6246 - dense_3_loss: 0.4258 - dense_1_acc: 0.7683 - dense_2_acc: 0.8747 - dense_3_acc: 0.9102"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2825/2825 [==============================] - 1450s 513ms/step - loss: 0.6813 - dense_1_loss: 0.8429 - dense_2_loss: 0.6170 - dense_3_loss: 0.4224 - dense_1_acc: 0.7736 - dense_2_acc: 0.8794 - dense_3_acc: 0.9121 - val_loss: 0.1001 - val_dense_1_loss: 0.1552 - val_dense_2_loss: 0.0544 - val_dense_3_loss: 0.0497 - val_dense_1_acc: 0.9592 - val_dense_2_acc: 0.9877 - val_dense_3_acc: 0.9876\n",
      "Epoch 20/80\n",
      "1072/2825 [==========>...................] - ETA: 14:21 - loss: 0.6797 - dense_1_loss: 0.8406 - dense_2_loss: 0.6161 - dense_3_loss: 0.4216 - dense_1_acc: 0.7716 - dense_2_acc: 0.8769 - dense_3_acc: 0.9108"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2825/2825 [==============================] - 1553s 550ms/step - loss: 0.6765 - dense_1_loss: 0.8356 - dense_2_loss: 0.6156 - dense_3_loss: 0.4193 - dense_1_acc: 0.7763 - dense_2_acc: 0.8789 - dense_3_acc: 0.9125 - val_loss: 0.0840 - val_dense_1_loss: 0.1593 - val_dense_2_loss: 0.0496 - val_dense_3_loss: 0.0504 - val_dense_1_acc: 0.9607 - val_dense_2_acc: 0.9884 - val_dense_3_acc: 0.9875\n",
      "Epoch 24/80\n",
      "1490/2825 [==============>...............] - ETA: 11:43 - loss: 0.6747 - dense_1_loss: 0.8331 - dense_2_loss: 0.6139 - dense_3_loss: 0.4190 - dense_1_acc: 0.7766 - dense_2_acc: 0.8794 - dense_3_acc: 0.9148"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2825/2825 [==============================] - 1552s 549ms/step - loss: 0.6726 - dense_1_loss: 0.8292 - dense_2_loss: 0.6135 - dense_3_loss: 0.4187 - dense_1_acc: 0.7796 - dense_2_acc: 0.8831 - dense_3_acc: 0.9158 - val_loss: 0.0819 - val_dense_1_loss: 0.1591 - val_dense_2_loss: 0.0575 - val_dense_3_loss: 0.0509 - val_dense_1_acc: 0.9591 - val_dense_2_acc: 0.9846 - val_dense_3_acc: 0.9878\n",
      "Epoch 28/80\n",
      "1407/2825 [=============>................] - ETA: 12:26 - loss: 0.6743 - dense_1_loss: 0.8296 - dense_2_loss: 0.6183 - dense_3_loss: 0.4199 - dense_1_acc: 0.7780 - dense_2_acc: 0.8775 - dense_3_acc: 0.9141"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2825/2825 [==============================] - 1553s 550ms/step - loss: 0.6678 - dense_1_loss: 0.8210 - dense_2_loss: 0.6120 - dense_3_loss: 0.4173 - dense_1_acc: 0.7825 - dense_2_acc: 0.8833 - dense_3_acc: 0.9156 - val_loss: 0.0249 - val_dense_1_loss: 0.1505 - val_dense_2_loss: 0.0484 - val_dense_3_loss: 0.0457 - val_dense_1_acc: 0.9611 - val_dense_2_acc: 0.9878 - val_dense_3_acc: 0.9886\n",
      "Epoch 32/80\n",
      "1814/2825 [==================>...........] - ETA: 8:52 - loss: 0.6688 - dense_1_loss: 0.8221 - dense_2_loss: 0.6121 - dense_3_loss: 0.4190 - dense_1_acc: 0.7801 - dense_2_acc: 0.8832 - dense_3_acc: 0.9165"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2825/2825 [==============================] - 1551s 549ms/step - loss: 0.6675 - dense_1_loss: 0.8202 - dense_2_loss: 0.6118 - dense_3_loss: 0.4179 - dense_1_acc: 0.7829 - dense_2_acc: 0.8843 - dense_3_acc: 0.9158 - val_loss: 0.0685 - val_dense_1_loss: 0.1624 - val_dense_2_loss: 0.0505 - val_dense_3_loss: 0.0467 - val_dense_1_acc: 0.9606 - val_dense_2_acc: 0.9884 - val_dense_3_acc: 0.9882\n",
      "Epoch 36/80\n",
      "2289/2825 [=======================>......] - ETA: 4:42 - loss: 0.6667 - dense_1_loss: 0.8194 - dense_2_loss: 0.6102 - dense_3_loss: 0.4177 - dense_1_acc: 0.7827 - dense_2_acc: 0.8852 - dense_3_acc: 0.9161"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2733/2825 [============================>.] - ETA: 48s - loss: 0.6609 - dense_1_loss: 0.8101 - dense_2_loss: 0.6084 - dense_3_loss: 0.4149 - dense_1_acc: 0.7866 - dense_2_acc: 0.8850 - dense_3_acc: 0.9175"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2704/2825 [===========================>..] - ETA: 1:03 - loss: 0.6588 - dense_1_loss: 0.8069 - dense_2_loss: 0.6068 - dense_3_loss: 0.4146 - dense_1_acc: 0.7876 - dense_2_acc: 0.8870 - dense_3_acc: 0.9181"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2825/2825 [==============================] - 1552s 549ms/step - loss: 0.6566 - dense_1_loss: 0.8035 - dense_2_loss: 0.6070 - dense_3_loss: 0.4125 - dense_1_acc: 0.7898 - dense_2_acc: 0.8875 - dense_3_acc: 0.9193 - val_loss: 0.0709 - val_dense_1_loss: 0.1610 - val_dense_2_loss: 0.0489 - val_dense_3_loss: 0.0432 - val_dense_1_acc: 0.9603 - val_dense_2_acc: 0.9880 - val_dense_3_acc: 0.9895\n",
      "Epoch 49/80\n",
      " 283/2825 [==>...........................] - ETA: 22:18 - loss: 0.6565 - dense_1_loss: 0.8003 - dense_2_loss: 0.6135 - dense_3_loss: 0.4122 - dense_1_acc: 0.7901 - dense_2_acc: 0.8904 - dense_3_acc: 0.9212"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2825/2825 [==============================] - 1551s 549ms/step - loss: 0.6541 - dense_1_loss: 0.7991 - dense_2_loss: 0.6057 - dense_3_loss: 0.4126 - dense_1_acc: 0.7898 - dense_2_acc: 0.8875 - dense_3_acc: 0.9205 - val_loss: 0.0641 - val_dense_1_loss: 0.1581 - val_dense_2_loss: 0.0488 - val_dense_3_loss: 0.0431 - val_dense_1_acc: 0.9598 - val_dense_2_acc: 0.9886 - val_dense_3_acc: 0.9895\n",
      "Epoch 53/80\n",
      " 758/2825 [=======>......................] - ETA: 18:12 - loss: 0.6536 - dense_1_loss: 0.7971 - dense_2_loss: 0.6059 - dense_3_loss: 0.4144 - dense_1_acc: 0.7884 - dense_2_acc: 0.8839 - dense_3_acc: 0.9189"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2825/2825 [==============================] - 1555s 550ms/step - loss: 0.6532 - dense_1_loss: 0.7978 - dense_2_loss: 0.6053 - dense_3_loss: 0.4119 - dense_1_acc: 0.7932 - dense_2_acc: 0.8898 - dense_3_acc: 0.9199 - val_loss: 0.0826 - val_dense_1_loss: 0.1634 - val_dense_2_loss: 0.0520 - val_dense_3_loss: 0.0460 - val_dense_1_acc: 0.9596 - val_dense_2_acc: 0.9884 - val_dense_3_acc: 0.9892\n",
      "Epoch 57/80\n",
      "2434/2825 [========================>.....] - ETA: 3:26 - loss: 0.6565 - dense_1_loss: 0.8027 - dense_2_loss: 0.6080 - dense_3_loss: 0.4125 - dense_1_acc: 0.7919 - dense_2_acc: 0.8879 - dense_3_acc: 0.9206"
     ]
    }
   ],
   "source": [
    "batch_size=64\n",
    "train_gen = DataLoader(X_train, y_train, training=True, batch_size=batch_size, size=size)\n",
    "valid_gen = DataLoader(X_test, y_test, training=False, batch_size=batch_size, size=size)\n",
    "#model = getMultiBased()\n",
    "#model.summary()\n",
    "#model.compile(optimizer=Adam(), metrics=[\"acc\"], loss=\"categorical_crossentropy\", loss_weights=[0.5,0.25,0.25])\n",
    "print(\"compiled\")\n",
    "#model=load_model(\"tmp-eff5-epoch0045.h5\")\n",
    "model.fit_generator(train_gen, validation_data=valid_gen, epochs=80, callbacks=[checkpoint], workers=64, use_multiprocessing=True)\n",
    "model.save(\"multiEfficientNetB5.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
