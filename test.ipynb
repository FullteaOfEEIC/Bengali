{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.models import load_model\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from collections import Counter\n",
    "import seaborn as sns\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import matplotlib.pyplot as plt\n",
    "import multiprocessing\n",
    "from multiprocessing import Pool\n",
    "import itertools\n",
    "from copy import deepcopy\n",
    "import random\n",
    "\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Flatten, Conv2D, MaxPool2D, BatchNormalization, GlobalAveragePooling2D, PReLU, GlobalMaxPooling2D\n",
    "from keras.optimizers import Adam\n",
    "from keras.utils import to_categorical, Sequence\n",
    "import tensorflow as tf\n",
    "from skimage.transform import AffineTransform, warp\n",
    "\n",
    "from datetime import datetime\n",
    "import pickle\n",
    "import scipy.stats as stats\n",
    "import os\n",
    "import sklearn.metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "_KERAS_BACKEND = None\n",
    "_KERAS_LAYERS = None\n",
    "_KERAS_MODELS = None\n",
    "_KERAS_UTILS = None\n",
    "\n",
    "\n",
    "def get_submodules_from_kwargs(kwargs):\n",
    "    backend = kwargs.get('backend', _KERAS_BACKEND)\n",
    "    layers = kwargs.get('layers', _KERAS_LAYERS)\n",
    "    models = kwargs.get('models', _KERAS_MODELS)\n",
    "    utils = kwargs.get('utils', _KERAS_UTILS)\n",
    "    for key in kwargs.keys():\n",
    "        if key not in ['backend', 'layers', 'models', 'utils']:\n",
    "            raise TypeError('Invalid keyword argument: %s', key)\n",
    "    return backend, layers, models, utils\n",
    "\n",
    "\n",
    "def inject_keras_modules(func):\n",
    "    import keras\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        kwargs['backend'] = keras.backend\n",
    "        kwargs['layers'] = keras.layers\n",
    "        kwargs['models'] = keras.models\n",
    "        kwargs['utils'] = keras.utils\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def inject_tfkeras_modules(func):\n",
    "    import tensorflow.keras as tfkeras\n",
    "    @functools.wraps(func)\n",
    "    def wrapper(*args, **kwargs):\n",
    "        kwargs['backend'] = tfkeras.backend\n",
    "        kwargs['layers'] = tfkeras.layers\n",
    "        kwargs['models'] = tfkeras.models\n",
    "        kwargs['utils'] = tfkeras.utils\n",
    "        return func(*args, **kwargs)\n",
    "\n",
    "    return wrapper\n",
    "\n",
    "\n",
    "def init_keras_custom_objects():\n",
    "    import keras\n",
    "    from . import model\n",
    "\n",
    "    custom_objects = {\n",
    "        'swish': inject_keras_modules(model.get_swish)(),\n",
    "        'FixedDropout': inject_keras_modules(model.get_dropout)()\n",
    "    }\n",
    "\n",
    "    keras.utils.generic_utils.get_custom_objects().update(custom_objects)\n",
    "\n",
    "\n",
    "def init_tfkeras_custom_objects():\n",
    "    import tensorflow.keras as tfkeras\n",
    "    from . import model\n",
    "\n",
    "    custom_objects = {\n",
    "        'swish': inject_tfkeras_modules(model.get_swish)(),\n",
    "        'FixedDropout': inject_tfkeras_modules(model.get_dropout)()\n",
    "    }\n",
    "\n",
    "    tfkeras.utils.get_custom_objects().update(custom_objects)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import string\n",
    "import collections\n",
    "import numpy as np\n",
    "\n",
    "from six.moves import xrange\n",
    "from keras_applications.imagenet_utils import _obtain_input_shape\n",
    "from keras_applications.imagenet_utils import decode_predictions\n",
    "from keras_applications.imagenet_utils import preprocess_input as _preprocess_input\n",
    "\n",
    "from . import get_submodules_from_kwargs\n",
    "\n",
    "backend = None\n",
    "layers = None\n",
    "models = None\n",
    "keras_utils = None\n",
    "\n",
    "BASE_WEIGHTS_PATH = (\n",
    "    'https://github.com/Callidior/keras-applications/'\n",
    "    'releases/download/efficientnet/')\n",
    "\n",
    "WEIGHTS_HASHES = {\n",
    "    'efficientnet-b0': ('163292582f1c6eaca8e7dc7b51b01c61'\n",
    "                        '5b0dbc0039699b4dcd0b975cc21533dc',\n",
    "                        'c1421ad80a9fc67c2cc4000f666aa507'\n",
    "                        '89ce39eedb4e06d531b0c593890ccff3'),\n",
    "    'efficientnet-b1': ('d0a71ddf51ef7a0ca425bab32b7fa7f1'\n",
    "                        '6043ee598ecee73fc674d9560c8f09b0',\n",
    "                        '75de265d03ac52fa74f2f510455ba64f'\n",
    "                        '9c7c5fd96dc923cd4bfefa3d680c4b68'),\n",
    "    'efficientnet-b2': ('bb5451507a6418a574534aa76a91b106'\n",
    "                        'f6b605f3b5dde0b21055694319853086',\n",
    "                        '433b60584fafba1ea3de07443b74cfd3'\n",
    "                        '2ce004a012020b07ef69e22ba8669333'),\n",
    "    'efficientnet-b3': ('03f1fba367f070bd2545f081cfa7f3e7'\n",
    "                        '6f5e1aa3b6f4db700f00552901e75ab9',\n",
    "                        'c5d42eb6cfae8567b418ad3845cfd63a'\n",
    "                        'a48b87f1bd5df8658a49375a9f3135c7'),\n",
    "    'efficientnet-b4': ('98852de93f74d9833c8640474b2c698d'\n",
    "                        'b45ec60690c75b3bacb1845e907bf94f',\n",
    "                        '7942c1407ff1feb34113995864970cd4'\n",
    "                        'd9d91ea64877e8d9c38b6c1e0767c411'),\n",
    "    'efficientnet-b5': ('30172f1d45f9b8a41352d4219bf930ee'\n",
    "                        '3339025fd26ab314a817ba8918fefc7d',\n",
    "                        '9d197bc2bfe29165c10a2af8c2ebc675'\n",
    "                        '07f5d70456f09e584c71b822941b1952'),\n",
    "    'efficientnet-b6': ('f5270466747753485a082092ac9939ca'\n",
    "                        'a546eb3f09edca6d6fff842cad938720',\n",
    "                        '1d0923bb038f2f8060faaf0a0449db4b'\n",
    "                        '96549a881747b7c7678724ac79f427ed'),\n",
    "    'efficientnet-b7': ('876a41319980638fa597acbbf956a82d'\n",
    "                        '10819531ff2dcb1a52277f10c7aefa1a',\n",
    "                        '60b56ff3a8daccc8d96edfd40b204c11'\n",
    "                        '3e51748da657afd58034d54d3cec2bac')\n",
    "}\n",
    "\n",
    "BlockArgs = collections.namedtuple('BlockArgs', [\n",
    "    'kernel_size', 'num_repeat', 'input_filters', 'output_filters',\n",
    "    'expand_ratio', 'id_skip', 'strides', 'se_ratio'\n",
    "])\n",
    "# defaults will be a public argument for namedtuple in Python 3.7\n",
    "# https://docs.python.org/3/library/collections.html#collections.namedtuple\n",
    "BlockArgs.__new__.__defaults__ = (None,) * len(BlockArgs._fields)\n",
    "\n",
    "DEFAULT_BLOCKS_ARGS = [\n",
    "    BlockArgs(kernel_size=3, num_repeat=1, input_filters=32, output_filters=16,\n",
    "              expand_ratio=1, id_skip=True, strides=[1, 1], se_ratio=0.25),\n",
    "    BlockArgs(kernel_size=3, num_repeat=2, input_filters=16, output_filters=24,\n",
    "              expand_ratio=6, id_skip=True, strides=[2, 2], se_ratio=0.25),\n",
    "    BlockArgs(kernel_size=5, num_repeat=2, input_filters=24, output_filters=40,\n",
    "              expand_ratio=6, id_skip=True, strides=[2, 2], se_ratio=0.25),\n",
    "    BlockArgs(kernel_size=3, num_repeat=3, input_filters=40, output_filters=80,\n",
    "              expand_ratio=6, id_skip=True, strides=[2, 2], se_ratio=0.25),\n",
    "    BlockArgs(kernel_size=5, num_repeat=3, input_filters=80, output_filters=112,\n",
    "              expand_ratio=6, id_skip=True, strides=[1, 1], se_ratio=0.25),\n",
    "    BlockArgs(kernel_size=5, num_repeat=4, input_filters=112, output_filters=192,\n",
    "              expand_ratio=6, id_skip=True, strides=[2, 2], se_ratio=0.25),\n",
    "    BlockArgs(kernel_size=3, num_repeat=1, input_filters=192, output_filters=320,\n",
    "              expand_ratio=6, id_skip=True, strides=[1, 1], se_ratio=0.25)\n",
    "]\n",
    "\n",
    "CONV_KERNEL_INITIALIZER = {\n",
    "    'class_name': 'VarianceScaling',\n",
    "    'config': {\n",
    "        'scale': 2.0,\n",
    "        'mode': 'fan_out',\n",
    "        # EfficientNet actually uses an untruncated normal distribution for\n",
    "        # initializing conv layers, but keras.initializers.VarianceScaling use\n",
    "        # a truncated distribution.\n",
    "        # We decided against a custom initializer for better serializability.\n",
    "        'distribution': 'normal'\n",
    "    }\n",
    "}\n",
    "\n",
    "DENSE_KERNEL_INITIALIZER = {\n",
    "    'class_name': 'VarianceScaling',\n",
    "    'config': {\n",
    "        'scale': 1. / 3.,\n",
    "        'mode': 'fan_out',\n",
    "        'distribution': 'uniform'\n",
    "    }\n",
    "}\n",
    "\n",
    "\n",
    "def preprocess_input(x, **kwargs):\n",
    "    kwargs = {k: v for k, v in kwargs.items() if k in ['backend', 'layers', 'models', 'utils']}\n",
    "    return _preprocess_input(x, mode='torch', **kwargs)\n",
    "\n",
    "\n",
    "def get_swish(**kwargs):\n",
    "    backend, layers, models, keras_utils = get_submodules_from_kwargs(kwargs)\n",
    "    def swish(x):\n",
    "        \"\"\"Swish activation function: x * sigmoid(x).\n",
    "        Reference: [Searching for Activation Functions](https://arxiv.org/abs/1710.05941)\n",
    "        \"\"\"\n",
    "\n",
    "        if backend.backend() == 'tensorflow':\n",
    "            try:\n",
    "                # The native TF implementation has a more\n",
    "                # memory-efficient gradient implementation\n",
    "                return backend.tf.nn.swish(x)\n",
    "            except AttributeError:\n",
    "                pass\n",
    "\n",
    "        return x * backend.sigmoid(x)\n",
    "    return  swish\n",
    "\n",
    "\n",
    "def get_dropout(**kwargs):\n",
    "    \"\"\"Wrapper over custom dropout. Fix problem of ``None`` shape for tf.keras.\n",
    "    It is not possible to define FixedDropout class as global object,\n",
    "    because we do not have modules for inheritance at first time.\n",
    "    Issue:\n",
    "        https://github.com/tensorflow/tensorflow/issues/30946\n",
    "    \"\"\"\n",
    "    backend, layers, models, keras_utils = get_submodules_from_kwargs(kwargs)\n",
    "\n",
    "    class FixedDropout(layers.Dropout):\n",
    "        def _get_noise_shape(self, inputs):\n",
    "            if self.noise_shape is None:\n",
    "                return self.noise_shape\n",
    "\n",
    "            symbolic_shape = backend.shape(inputs)\n",
    "            noise_shape = [symbolic_shape[axis] if shape is None else shape\n",
    "                           for axis, shape in enumerate(self.noise_shape)]\n",
    "            return tuple(noise_shape)\n",
    "\n",
    "    return FixedDropout\n",
    "\n",
    "\n",
    "def round_filters(filters, width_coefficient, depth_divisor):\n",
    "    \"\"\"Round number of filters based on width multiplier.\"\"\"\n",
    "\n",
    "    filters *= width_coefficient\n",
    "    new_filters = int(filters + depth_divisor / 2) // depth_divisor * depth_divisor\n",
    "    new_filters = max(depth_divisor, new_filters)\n",
    "    # Make sure that round down does not go down by more than 10%.\n",
    "    if new_filters < 0.9 * filters:\n",
    "        new_filters += depth_divisor\n",
    "    return int(new_filters)\n",
    "\n",
    "\n",
    "def round_repeats(repeats, depth_coefficient):\n",
    "    \"\"\"Round number of repeats based on depth multiplier.\"\"\"\n",
    "\n",
    "    return int(math.ceil(depth_coefficient * repeats))\n",
    "\n",
    "\n",
    "def mb_conv_block(inputs, block_args, activation, drop_rate=None, prefix='', ):\n",
    "    \"\"\"Mobile Inverted Residual Bottleneck.\"\"\"\n",
    "\n",
    "    has_se = (block_args.se_ratio is not None) and (0 < block_args.se_ratio <= 1)\n",
    "    bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1\n",
    "\n",
    "    # workaround over non working dropout with None in noise_shape in tf.keras\n",
    "    Dropout = get_dropout(\n",
    "        backend=backend,\n",
    "        layers=layers,\n",
    "        models=models,\n",
    "        utils=keras_utils\n",
    "    )\n",
    "\n",
    "    # Expansion phase\n",
    "    filters = block_args.input_filters * block_args.expand_ratio\n",
    "    if block_args.expand_ratio != 1:\n",
    "        x = layers.Conv2D(filters, 1,\n",
    "                          padding='same',\n",
    "                          use_bias=False,\n",
    "                          kernel_initializer=CONV_KERNEL_INITIALIZER,\n",
    "                          name=prefix + 'expand_conv')(inputs)\n",
    "        x = layers.BatchNormalization(axis=bn_axis, name=prefix + 'expand_bn')(x)\n",
    "        x = layers.Activation(activation, name=prefix + 'expand_activation')(x)\n",
    "    else:\n",
    "        x = inputs\n",
    "\n",
    "    # Depthwise Convolution\n",
    "    x = layers.DepthwiseConv2D(block_args.kernel_size,\n",
    "                               strides=block_args.strides,\n",
    "                               padding='same',\n",
    "                               use_bias=False,\n",
    "                               depthwise_initializer=CONV_KERNEL_INITIALIZER,\n",
    "                               name=prefix + 'dwconv')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=prefix + 'bn')(x)\n",
    "    x = layers.Activation(activation, name=prefix + 'activation')(x)\n",
    "\n",
    "    # Squeeze and Excitation phase\n",
    "    if has_se:\n",
    "        num_reduced_filters = max(1, int(\n",
    "            block_args.input_filters * block_args.se_ratio\n",
    "        ))\n",
    "        se_tensor = layers.GlobalAveragePooling2D(name=prefix + 'se_squeeze')(x)\n",
    "\n",
    "        target_shape = (1, 1, filters) if backend.image_data_format() == 'channels_last' else (filters, 1, 1)\n",
    "        se_tensor = layers.Reshape(target_shape, name=prefix + 'se_reshape')(se_tensor)\n",
    "        se_tensor = layers.Conv2D(num_reduced_filters, 1,\n",
    "                                  activation=activation,\n",
    "                                  padding='same',\n",
    "                                  use_bias=True,\n",
    "                                  kernel_initializer=CONV_KERNEL_INITIALIZER,\n",
    "                                  name=prefix + 'se_reduce')(se_tensor)\n",
    "        se_tensor = layers.Conv2D(filters, 1,\n",
    "                                  activation='sigmoid',\n",
    "                                  padding='same',\n",
    "                                  use_bias=True,\n",
    "                                  kernel_initializer=CONV_KERNEL_INITIALIZER,\n",
    "                                  name=prefix + 'se_expand')(se_tensor)\n",
    "        if backend.backend() == 'theano':\n",
    "            # For the Theano backend, we have to explicitly make\n",
    "            # the excitation weights broadcastable.\n",
    "            pattern = ([True, True, True, False] if backend.image_data_format() == 'channels_last'\n",
    "                       else [True, False, True, True])\n",
    "            se_tensor = layers.Lambda(\n",
    "                lambda x: backend.pattern_broadcast(x, pattern),\n",
    "                name=prefix + 'se_broadcast')(se_tensor)\n",
    "        x = layers.multiply([x, se_tensor], name=prefix + 'se_excite')\n",
    "\n",
    "    # Output phase\n",
    "    x = layers.Conv2D(block_args.output_filters, 1,\n",
    "                      padding='same',\n",
    "                      use_bias=False,\n",
    "                      kernel_initializer=CONV_KERNEL_INITIALIZER,\n",
    "                      name=prefix + 'project_conv')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name=prefix + 'project_bn')(x)\n",
    "    if block_args.id_skip and all(\n",
    "            s == 1 for s in block_args.strides\n",
    "    ) and block_args.input_filters == block_args.output_filters:\n",
    "        if drop_rate and (drop_rate > 0):\n",
    "            x = Dropout(drop_rate,\n",
    "                        noise_shape=(None, 1, 1, 1),\n",
    "                        name=prefix + 'drop')(x)\n",
    "        x = layers.add([x, inputs], name=prefix + 'add')\n",
    "\n",
    "    return x\n",
    "\n",
    "\n",
    "def EfficientNet(width_coefficient,\n",
    "                 depth_coefficient,\n",
    "                 default_resolution,\n",
    "                 dropout_rate=0.2,\n",
    "                 drop_connect_rate=0.2,\n",
    "                 depth_divisor=8,\n",
    "                 blocks_args=DEFAULT_BLOCKS_ARGS,\n",
    "                 model_name='efficientnet',\n",
    "                 include_top=True,\n",
    "                 weights='imagenet',\n",
    "                 input_tensor=None,\n",
    "                 input_shape=None,\n",
    "                 pooling=None,\n",
    "                 classes=1000,\n",
    "                 **kwargs):\n",
    "    \"\"\"Instantiates the EfficientNet architecture using given scaling coefficients.\n",
    "    Optionally loads weights pre-trained on ImageNet.\n",
    "    Note that the data format convention used by the model is\n",
    "    the one specified in your Keras config at `~/.keras/keras.json`.\n",
    "    # Arguments\n",
    "        width_coefficient: float, scaling coefficient for network width.\n",
    "        depth_coefficient: float, scaling coefficient for network depth.\n",
    "        default_resolution: int, default input image size.\n",
    "        dropout_rate: float, dropout rate before final classifier layer.\n",
    "        drop_connect_rate: float, dropout rate at skip connections.\n",
    "        depth_divisor: int.\n",
    "        blocks_args: A list of BlockArgs to construct block modules.\n",
    "        model_name: string, model name.\n",
    "        include_top: whether to include the fully-connected\n",
    "            layer at the top of the network.\n",
    "        weights: one of `None` (random initialization),\n",
    "              'imagenet' (pre-training on ImageNet),\n",
    "              or the path to the weights file to be loaded.\n",
    "        input_tensor: optional Keras tensor\n",
    "            (i.e. output of `layers.Input()`)\n",
    "            to use as image input for the model.\n",
    "        input_shape: optional shape tuple, only to be specified\n",
    "            if `include_top` is False.\n",
    "            It should have exactly 3 inputs channels.\n",
    "        pooling: optional pooling mode for feature extraction\n",
    "            when `include_top` is `False`.\n",
    "            - `None` means that the output of the model will be\n",
    "                the 4D tensor output of the\n",
    "                last convolutional layer.\n",
    "            - `avg` means that global average pooling\n",
    "                will be applied to the output of the\n",
    "                last convolutional layer, and thus\n",
    "                the output of the model will be a 2D tensor.\n",
    "            - `max` means that global max pooling will\n",
    "                be applied.\n",
    "        classes: optional number of classes to classify images\n",
    "            into, only to be specified if `include_top` is True, and\n",
    "            if no `weights` argument is specified.\n",
    "    # Returns\n",
    "        A Keras model instance.\n",
    "    # Raises\n",
    "        ValueError: in case of invalid argument for `weights`,\n",
    "            or invalid input shape.\n",
    "    \"\"\"\n",
    "    global backend, layers, models, keras_utils\n",
    "    backend, layers, models, keras_utils = get_submodules_from_kwargs(kwargs)\n",
    "\n",
    "    if not (weights in {'imagenet', None} or os.path.exists(weights)):\n",
    "        raise ValueError('The `weights` argument should be either '\n",
    "                         '`None` (random initialization), `imagenet` '\n",
    "                         '(pre-training on ImageNet), '\n",
    "                         'or the path to the weights file to be loaded.')\n",
    "\n",
    "    if weights == 'imagenet' and include_top and classes != 1000:\n",
    "        raise ValueError('If using `weights` as `\"imagenet\"` with `include_top`'\n",
    "                         ' as true, `classes` should be 1000')\n",
    "\n",
    "    # Determine proper input shape\n",
    "    input_shape = _obtain_input_shape(input_shape,\n",
    "                                      default_size=default_resolution,\n",
    "                                      min_size=32,\n",
    "                                      data_format=backend.image_data_format(),\n",
    "                                      require_flatten=include_top,\n",
    "                                      weights=weights)\n",
    "\n",
    "    if input_tensor is None:\n",
    "        img_input = layers.Input(shape=input_shape)\n",
    "    else:\n",
    "        if backend.backend() == 'tensorflow':\n",
    "            from tensorflow.python.keras.backend import is_keras_tensor\n",
    "        else:\n",
    "            is_keras_tensor = backend.is_keras_tensor\n",
    "        if not is_keras_tensor(input_tensor):\n",
    "            img_input = layers.Input(tensor=input_tensor, shape=input_shape)\n",
    "        else:\n",
    "            img_input = input_tensor\n",
    "\n",
    "    bn_axis = 3 if backend.image_data_format() == 'channels_last' else 1\n",
    "    activation = get_swish(**kwargs)\n",
    "\n",
    "    # Build stem\n",
    "    x = img_input\n",
    "    x = layers.Conv2D(round_filters(32, width_coefficient, depth_divisor), 3,\n",
    "                      strides=(2, 2),\n",
    "                      padding='same',\n",
    "                      use_bias=False,\n",
    "                      kernel_initializer=CONV_KERNEL_INITIALIZER,\n",
    "                      name='stem_conv')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name='stem_bn')(x)\n",
    "    x = layers.Activation(activation, name='stem_activation')(x)\n",
    "\n",
    "    # Build blocks\n",
    "    num_blocks_total = sum(block_args.num_repeat for block_args in blocks_args)\n",
    "    block_num = 0\n",
    "    for idx, block_args in enumerate(blocks_args):\n",
    "        assert block_args.num_repeat > 0\n",
    "        # Update block input and output filters based on depth multiplier.\n",
    "        block_args = block_args._replace(\n",
    "            input_filters=round_filters(block_args.input_filters,\n",
    "                                        width_coefficient, depth_divisor),\n",
    "            output_filters=round_filters(block_args.output_filters,\n",
    "                                         width_coefficient, depth_divisor),\n",
    "            num_repeat=round_repeats(block_args.num_repeat, depth_coefficient))\n",
    "\n",
    "        # The first block needs to take care of stride and filter size increase.\n",
    "        drop_rate = drop_connect_rate * float(block_num) / num_blocks_total\n",
    "        x = mb_conv_block(x, block_args,\n",
    "                          activation=activation,\n",
    "                          drop_rate=drop_rate,\n",
    "                          prefix='block{}a_'.format(idx + 1))\n",
    "        block_num += 1\n",
    "        if block_args.num_repeat > 1:\n",
    "            # pylint: disable=protected-access\n",
    "            block_args = block_args._replace(\n",
    "                input_filters=block_args.output_filters, strides=[1, 1])\n",
    "            # pylint: enable=protected-access\n",
    "            for bidx in xrange(block_args.num_repeat - 1):\n",
    "                drop_rate = drop_connect_rate * float(block_num) / num_blocks_total\n",
    "                block_prefix = 'block{}{}_'.format(\n",
    "                    idx + 1,\n",
    "                    string.ascii_lowercase[bidx + 1]\n",
    "                )\n",
    "                x = mb_conv_block(x, block_args,\n",
    "                                  activation=activation,\n",
    "                                  drop_rate=drop_rate,\n",
    "                                  prefix=block_prefix)\n",
    "                block_num += 1\n",
    "\n",
    "    # Build top\n",
    "    x = layers.Conv2D(round_filters(1280, width_coefficient, depth_divisor), 1,\n",
    "                      padding='same',\n",
    "                      use_bias=False,\n",
    "                      kernel_initializer=CONV_KERNEL_INITIALIZER,\n",
    "                      name='top_conv')(x)\n",
    "    x = layers.BatchNormalization(axis=bn_axis, name='top_bn')(x)\n",
    "    x = layers.Activation(activation, name='top_activation')(x)\n",
    "    if include_top:\n",
    "        x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "        if dropout_rate and dropout_rate > 0:\n",
    "            x = layers.Dropout(dropout_rate, name='top_dropout')(x)\n",
    "        x = layers.Dense(classes,\n",
    "                         activation='softmax',\n",
    "                         kernel_initializer=DENSE_KERNEL_INITIALIZER,\n",
    "                         name='probs')(x)\n",
    "    else:\n",
    "        if pooling == 'avg':\n",
    "            x = layers.GlobalAveragePooling2D(name='avg_pool')(x)\n",
    "        elif pooling == 'max':\n",
    "            x = layers.GlobalMaxPooling2D(name='max_pool')(x)\n",
    "\n",
    "    # Ensure that the model takes into account\n",
    "    # any potential predecessors of `input_tensor`.\n",
    "    if input_tensor is not None:\n",
    "        inputs = keras_utils.get_source_inputs(input_tensor)\n",
    "    else:\n",
    "        inputs = img_input\n",
    "\n",
    "    # Create model.\n",
    "    model = models.Model(inputs, x, name=model_name)\n",
    "\n",
    "    # Load weights.\n",
    "    if weights == 'imagenet':\n",
    "        if include_top:\n",
    "            file_name = model_name + '_weights_tf_dim_ordering_tf_kernels_autoaugment.h5'\n",
    "            file_hash = WEIGHTS_HASHES[model_name][0]\n",
    "        else:\n",
    "            file_name = model_name + '_weights_tf_dim_ordering_tf_kernels_autoaugment_notop.h5'\n",
    "            file_hash = WEIGHTS_HASHES[model_name][1]\n",
    "        weights_path = keras_utils.get_file(file_name,\n",
    "                                            BASE_WEIGHTS_PATH + file_name,\n",
    "                                            cache_subdir='models',\n",
    "                                            file_hash=file_hash)\n",
    "        model.load_weights(weights_path)\n",
    "    elif weights is not None:\n",
    "        model.load_weights(weights)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "def EfficientNetB0(include_top=True,\n",
    "                   weights='imagenet',\n",
    "                   input_tensor=None,\n",
    "                   input_shape=None,\n",
    "                   pooling=None,\n",
    "                   classes=1000,\n",
    "                   **kwargs):\n",
    "    return EfficientNet(1.0, 1.0, 224, 0.2,\n",
    "                        model_name='efficientnet-b0',\n",
    "                        include_top=include_top, weights=weights,\n",
    "                        input_tensor=input_tensor, input_shape=input_shape,\n",
    "                        pooling=pooling, classes=classes,\n",
    "                        **kwargs)\n",
    "\n",
    "\n",
    "def EfficientNetB1(include_top=True,\n",
    "                   weights='imagenet',\n",
    "                   input_tensor=None,\n",
    "                   input_shape=None,\n",
    "                   pooling=None,\n",
    "                   classes=1000,\n",
    "                   **kwargs):\n",
    "    return EfficientNet(1.0, 1.1, 240, 0.2,\n",
    "                        model_name='efficientnet-b1',\n",
    "                        include_top=include_top, weights=weights,\n",
    "                        input_tensor=input_tensor, input_shape=input_shape,\n",
    "                        pooling=pooling, classes=classes,\n",
    "                        **kwargs)\n",
    "\n",
    "\n",
    "def EfficientNetB2(include_top=True,\n",
    "                   weights='imagenet',\n",
    "                   input_tensor=None,\n",
    "                   input_shape=None,\n",
    "                   pooling=None,\n",
    "                   classes=1000,\n",
    "                   **kwargs):\n",
    "    return EfficientNet(1.1, 1.2, 260, 0.3,\n",
    "                        model_name='efficientnet-b2',\n",
    "                        include_top=include_top, weights=weights,\n",
    "                        input_tensor=input_tensor, input_shape=input_shape,\n",
    "                        pooling=pooling, classes=classes,\n",
    "                        **kwargs)\n",
    "\n",
    "\n",
    "def EfficientNetB3(include_top=True,\n",
    "                   weights='imagenet',\n",
    "                   input_tensor=None,\n",
    "                   input_shape=None,\n",
    "                   pooling=None,\n",
    "                   classes=1000,\n",
    "                   **kwargs):\n",
    "    return EfficientNet(1.2, 1.4, 300, 0.3,\n",
    "                        model_name='efficientnet-b3',\n",
    "                        include_top=include_top, weights=weights,\n",
    "                        input_tensor=input_tensor, input_shape=input_shape,\n",
    "                        pooling=pooling, classes=classes,\n",
    "                        **kwargs)\n",
    "\n",
    "\n",
    "def EfficientNetB4(include_top=True,\n",
    "                   weights='imagenet',\n",
    "                   input_tensor=None,\n",
    "                   input_shape=None,\n",
    "                   pooling=None,\n",
    "                   classes=1000,\n",
    "                   **kwargs):\n",
    "    return EfficientNet(1.4, 1.8, 380, 0.4,\n",
    "                        model_name='efficientnet-b4',\n",
    "                        include_top=include_top, weights=weights,\n",
    "                        input_tensor=input_tensor, input_shape=input_shape,\n",
    "                        pooling=pooling, classes=classes,\n",
    "                        **kwargs)\n",
    "\n",
    "\n",
    "def EfficientNetB5(include_top=True,\n",
    "                   weights='imagenet',\n",
    "                   input_tensor=None,\n",
    "                   input_shape=None,\n",
    "                   pooling=None,\n",
    "                   classes=1000,\n",
    "                   **kwargs):\n",
    "    return EfficientNet(1.6, 2.2, 456, 0.4,\n",
    "                        model_name='efficientnet-b5',\n",
    "                        include_top=include_top, weights=weights,\n",
    "                        input_tensor=input_tensor, input_shape=input_shape,\n",
    "                        pooling=pooling, classes=classes,\n",
    "                        **kwargs)\n",
    "\n",
    "\n",
    "def EfficientNetB6(include_top=True,\n",
    "                   weights='imagenet',\n",
    "                   input_tensor=None,\n",
    "                   input_shape=None,\n",
    "                   pooling=None,\n",
    "                   classes=1000,\n",
    "                   **kwargs):\n",
    "    return EfficientNet(1.8, 2.6, 528, 0.5,\n",
    "                        model_name='efficientnet-b6',\n",
    "                        include_top=include_top, weights=weights,\n",
    "                        input_tensor=input_tensor, input_shape=input_shape,\n",
    "                        pooling=pooling, classes=classes,\n",
    "                        **kwargs)\n",
    "\n",
    "\n",
    "def EfficientNetB7(include_top=True,\n",
    "                   weights='imagenet',\n",
    "                   input_tensor=None,\n",
    "                   input_shape=None,\n",
    "                   pooling=None,\n",
    "                   classes=1000,\n",
    "                   **kwargs):\n",
    "    return EfficientNet(2.0, 3.1, 600, 0.5,\n",
    "                        model_name='efficientnet-b7',\n",
    "                        include_top=include_top, weights=weights,\n",
    "                        input_tensor=input_tensor, input_shape=input_shape,\n",
    "                        pooling=pooling, classes=classes,\n",
    "                        **kwargs)\n",
    "\n",
    "\n",
    "setattr(EfficientNetB0, '__doc__', EfficientNet.__doc__)\n",
    "setattr(EfficientNetB1, '__doc__', EfficientNet.__doc__)\n",
    "setattr(EfficientNetB2, '__doc__', EfficientNet.__doc__)\n",
    "setattr(EfficientNetB3, '__doc__', EfficientNet.__doc__)\n",
    "setattr(EfficientNetB4, '__doc__', EfficientNet.__doc__)\n",
    "setattr(EfficientNetB5, '__doc__', EfficientNet.__doc__)\n",
    "setattr(EfficientNetB6, '__doc__', EfficientNet.__doc__)\n",
    "setattr(EfficientNetB7, '__doc__', EfficientNet.__doc__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=load_model(\"tmp-eff2-epoch0001.h5\",custom_objects = {\n",
    "        'swish': inject_keras_modules(get_swish)(),\n",
    "        'FixedDropout': inject_keras_modules(get_dropout)()\n",
    "    }\n",
    "    ,compile=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = pd.read_csv(\"data/train.csv\")\n",
    "yEval = pd.read_csv(\"data/test.csv\")\n",
    "classMap = pd.read_csv(\"data/class_map.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "tables = [pq.read_table('data/train_image_data_{0}.parquet'.format(i)).to_pandas() for i in range(1)]\n",
    "df = pd.concat(tables)\n",
    "df = df.set_index(\"image_id\")\n",
    "del tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df.values.reshape(-1,137,236),y[:df.shape[0]],train_size=0.9,random_state=8000)\n",
    "del df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataLoader(Sequence):\n",
    "    def __init__(self,X,y,training,batch_size=64,size=(255,255),alpha=3):\n",
    "        self.training = training\n",
    "        self.batch_size=batch_size\n",
    "        self.X=X\n",
    "        self.y=y\n",
    "        self.size=size\n",
    "        self.alpha=alpha\n",
    "\n",
    "    def __len__(self):\n",
    "        return int(np.ceil(self.X.shape[0] / self.batch_size))\n",
    "\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        _imgs=self.X[idx * self.batch_size:(idx + 1) * self.batch_size,:,:]\n",
    "        \n",
    "            \n",
    "        \n",
    "        imgs=[]\n",
    "        for img in _imgs:\n",
    "            imgs.append(transformImg(img,size=self.size,training=self.training))\n",
    "         \n",
    "        \n",
    "        \n",
    "        imgs=np.asarray(imgs)\n",
    "        \n",
    "        \n",
    "        ret_y=[]\n",
    "        for label in labels:\n",
    "            ret_y.append(to_categorical(self.y[idx * self.batch_size:(idx + 1) * self.batch_size][label],num_classes=len(set(y[label]))))\n",
    "    \n",
    "    \n",
    "        #mix up\n",
    "        if self.training :\n",
    "            r = np.random.permutation(imgs.shape[0])\n",
    "            imgs2=deepcopy(imgs)[r]\n",
    "            grapheme=ret_y[0]\n",
    "            vowel=ret_y[1]\n",
    "            consonant=ret_y[2]\n",
    "            grapheme2=deepcopy(grapheme)[r]\n",
    "            vowel2=deepcopy(vowel)[r]\n",
    "            consonant2=deepcopy(consonant)[r]\n",
    "            ratio=np.random.beta(self.alpha,self.alpha,imgs.shape[0])\n",
    "            ratio[ratio>1]=1\n",
    "            ratio[ratio<0]=0\n",
    "            imgs=np.tile(ratio,(3,*size,1)).T*imgs+np.tile((1-ratio),(3,*size,1)).T*imgs2\n",
    "            grapheme=np.tile(ratio,(168,1)).T*grapheme+np.tile((1-ratio),(168,1)).T*grapheme2\n",
    "            vowel=np.tile(ratio,(11,1)).T*vowel+np.tile((1-ratio),(11,1)).T*vowel2\n",
    "            consonant=np.tile(ratio,(7,1)).T*consonant+np.tile((1-ratio),(7,1)).T*consonant2\n",
    "            grapheme=grapheme.astype(np.float32)\n",
    "            vowel=vowel.astype(np.float32)\n",
    "            consonant=consonant.astype(np.float32)\n",
    "            ret_y=[grapheme,vowel,consonant]\n",
    "   \n",
    "        if self.training:\n",
    "            #imgs = [randomErase(img) for img in imgs]\n",
    "            pass\n",
    "            \n",
    "            \n",
    "        imgs = np.asarray(imgs).astype(np.float32)/255.0\n",
    "            \n",
    "\n",
    "        return imgs, ret_y\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "size=(120, 120)\n",
    "\n",
    "def calcRotate(img):\n",
    "    detector = cv2.ORB_create()\n",
    "    keypoints=detector.detect(img)\n",
    "    descriptors=detector.compute(img,keypoints)\n",
    "    angles=[]\n",
    "    weights=[]\n",
    "    for i in descriptors[0]:\n",
    "        if i.angle!=-1:\n",
    "            angles.append(i.angle)\n",
    "            weights.append(i.response)\n",
    "    if len(angles)==0:\n",
    "        return 0\n",
    "    else:\n",
    "        return np.average(angles,weights=weights)\n",
    "\n",
    "def upper(img,mergin=20):\n",
    "    up=0\n",
    "    bottom=img.shape[0]\n",
    "    while bottom-up>2:\n",
    "        mid=(up+bottom)//2\n",
    "        if np.sum(img[up:mid,:])==0:\n",
    "            up=mid\n",
    "        else:\n",
    "            bottom=mid\n",
    "    return max(up-mergin,0)\n",
    "\n",
    "def lower(img,mergin=20):\n",
    "    up=0\n",
    "    bottom=img.shape[0]\n",
    "    while bottom-up>2:\n",
    "        mid=(up+bottom)//2\n",
    "        if np.sum(img[mid:bottom,:])==0:\n",
    "            bottom=mid\n",
    "        else:\n",
    "            up=mid\n",
    "    return min(bottom+mergin,img.shape[0])\n",
    "\n",
    "def lefter(img,mergin=20):\n",
    "    left=0\n",
    "    right=img.shape[1]\n",
    "    while right-left>2:\n",
    "        mid=(left+right)//2\n",
    "        if np.sum(img[:,left:mid])==0:\n",
    "            left=mid\n",
    "        else:\n",
    "            right=mid\n",
    "    return max(left-mergin,0)\n",
    "\n",
    "def righter(img,mergin=20):\n",
    "    left=0\n",
    "    right=img.shape[1]\n",
    "    while right-left>2:\n",
    "        mid=(left+right)//2\n",
    "        if np.sum(img[:,mid:right])==0:\n",
    "            right=mid\n",
    "        else:\n",
    "            left=mid\n",
    "    return min(right+mergin,img.shape[1])\n",
    "\n",
    "def transformImg(img,size=(255,255),training=True):\n",
    "    ret2, img = cv2.threshold(img, 0, 255, cv2.THRESH_OTSU)\n",
    "    img = 255-img\n",
    "    if training:\n",
    "        img=affine_image(img)\n",
    "    img = img[upper(img):lower(img),lefter(img):righter(img)]\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_GRAY2RGB)\n",
    "    img = cv2.resize(img, (size[0],size[1]))\n",
    "    kernel = np.ones((3,3),np.float32)/9\n",
    "    img = cv2.filter2D(img,-1,kernel)\n",
    "    return img\n",
    "\n",
    "def randomErase(img, prob=True):\n",
    "    # random erasing\n",
    "    # https://github.com/yu4u/cutout-random-erasing\n",
    "    p = 0.5\n",
    "    s_l = 0.02\n",
    "    s_h = 0.4\n",
    "    r_1 = 0.3\n",
    "    r_2 = 1 / 0.3\n",
    "    v_l = 0\n",
    "    v_h = 255\n",
    "    input_size=size[0]\n",
    "    if prob==False or np.random.random()<p:\n",
    "\n",
    "        while True:\n",
    "            s = np.random.uniform(s_l, s_h) * input_size * input_size\n",
    "            r = np.random.uniform(r_1, r_2)\n",
    "            w = int(np.sqrt(s / r))\n",
    "            h = int(np.sqrt(s * r))\n",
    "            left = np.random.randint(0, input_size)\n",
    "            top = np.random.randint(0, input_size)\n",
    "            if left + w <= input_size and top + h <= input_size:\n",
    "                break\n",
    "        c = np.random.uniform(v_l, v_h, (h, w, 3))\n",
    "        img[top : top + h, left : left + w, :] = c\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = [\"grapheme_root\",\"vowel_diacritic\",\"consonant_diacritic\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "valid_gen = DataLoader(X_test, y_test, training=False, batch_size=128, size=(80,80))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dimension(80)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.input.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40/40 [==============================] - 55s 1s/step\n"
     ]
    }
   ],
   "source": [
    "y_pred=model.predict_generator(valid_gen, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_preds={\"grapheme_root\":[],\"vowel_diacritic\":[],\"consonant_diacritic\":[]}\n",
    "for j,label in enumerate([\"grapheme_root\",\"vowel_diacritic\",\"consonant_diacritic\"]):\n",
    "    tmp = y_preds[label]\n",
    "    tmp.append(y_pred[j])\n",
    "    y_preds[label]=tmp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "ans = {}\n",
    "for key in y_preds:\n",
    "        y_pred = np.asarray(y_preds[key])\n",
    "        #y_pred, _ = stats.mode(y_preds,axis=0)\n",
    "        #y_pred = y_pred.reshape(-1,)\n",
    "        y_pred = np.mean(y_preds[key],axis=0)\n",
    "        y_pred = np.argmax(y_pred, axis=1)\n",
    "        ans[key] = y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9551882095200159"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l=\"consonant_diacritic\"\n",
    "accuracy_score(ans[l], y_test[l])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image_id</th>\n",
       "      <th>grapheme_root</th>\n",
       "      <th>vowel_diacritic</th>\n",
       "      <th>consonant_diacritic</th>\n",
       "      <th>grapheme</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>35486</th>\n",
       "      <td>Train_35486</td>\n",
       "      <td>101</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>প্লা</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9103</th>\n",
       "      <td>Train_9103</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ঋ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40498</th>\n",
       "      <td>Train_40498</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>স্ট</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29403</th>\n",
       "      <td>Train_29403</td>\n",
       "      <td>144</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>ষ্পে</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16191</th>\n",
       "      <td>Train_16191</td>\n",
       "      <td>107</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>বে</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19359</th>\n",
       "      <td>Train_19359</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>পা</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28480</th>\n",
       "      <td>Train_28480</td>\n",
       "      <td>79</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ধা</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34491</th>\n",
       "      <td>Train_34491</td>\n",
       "      <td>103</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>ফে</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25116</th>\n",
       "      <td>Train_25116</td>\n",
       "      <td>80</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>র্ধ্বে</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19475</th>\n",
       "      <td>Train_19475</td>\n",
       "      <td>121</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>ম্লে</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40616</th>\n",
       "      <td>Train_40616</td>\n",
       "      <td>141</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>ষ্ট্যে</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6096</th>\n",
       "      <td>Train_6096</td>\n",
       "      <td>113</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>ভো</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12028</th>\n",
       "      <td>Train_12028</td>\n",
       "      <td>64</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>তা</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6983</th>\n",
       "      <td>Train_6983</td>\n",
       "      <td>62</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>ণ্ডা</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42458</th>\n",
       "      <td>Train_42458</td>\n",
       "      <td>72</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>র্দা</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45570</th>\n",
       "      <td>Train_45570</td>\n",
       "      <td>95</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ন্স</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37821</th>\n",
       "      <td>Train_37821</td>\n",
       "      <td>122</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>য</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17876</th>\n",
       "      <td>Train_17876</td>\n",
       "      <td>61</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>ণ্ঠে</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32581</th>\n",
       "      <td>Train_32581</td>\n",
       "      <td>98</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>প্তি</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31253</th>\n",
       "      <td>Train_31253</td>\n",
       "      <td>162</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>হ্ম</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19141</th>\n",
       "      <td>Train_19141</td>\n",
       "      <td>165</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>ড়ী</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1696</th>\n",
       "      <td>Train_1696</td>\n",
       "      <td>96</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>পৈ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7524</th>\n",
       "      <td>Train_7524</td>\n",
       "      <td>18</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>ক্ষী</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14210</th>\n",
       "      <td>Train_14210</td>\n",
       "      <td>165</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>ড়ু</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25742</th>\n",
       "      <td>Train_25742</td>\n",
       "      <td>127</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>ল্টো</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43440</th>\n",
       "      <td>Train_43440</td>\n",
       "      <td>159</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>হু</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50163</th>\n",
       "      <td>Train_50163</td>\n",
       "      <td>151</td>\n",
       "      <td>7</td>\n",
       "      <td>4</td>\n",
       "      <td>স্থ্যে</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40888</th>\n",
       "      <td>Train_40888</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ক</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33397</th>\n",
       "      <td>Train_33397</td>\n",
       "      <td>94</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>ন্মু</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3100</th>\n",
       "      <td>Train_3100</td>\n",
       "      <td>111</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>র্ব্ব</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40980</th>\n",
       "      <td>Train_40980</td>\n",
       "      <td>113</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>ভু</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43894</th>\n",
       "      <td>Train_43894</td>\n",
       "      <td>85</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>ন্ডো</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31804</th>\n",
       "      <td>Train_31804</td>\n",
       "      <td>113</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>র্ভু</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2148</th>\n",
       "      <td>Train_2148</td>\n",
       "      <td>125</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>ল্কি</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12548</th>\n",
       "      <td>Train_12548</td>\n",
       "      <td>68</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ত্ন</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15114</th>\n",
       "      <td>Train_15114</td>\n",
       "      <td>55</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>ঠে</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41939</th>\n",
       "      <td>Train_41939</td>\n",
       "      <td>16</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>ক্তি</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4819</th>\n",
       "      <td>Train_4819</td>\n",
       "      <td>113</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>র্ভ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19458</th>\n",
       "      <td>Train_19458</td>\n",
       "      <td>124</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>লী</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18449</th>\n",
       "      <td>Train_18449</td>\n",
       "      <td>66</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>ত্ত্বী</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14737</th>\n",
       "      <td>Train_14737</td>\n",
       "      <td>79</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "      <td>র্ধে</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44875</th>\n",
       "      <td>Train_44875</td>\n",
       "      <td>22</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>খৈ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43562</th>\n",
       "      <td>Train_43562</td>\n",
       "      <td>77</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>দ্ভি</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46456</th>\n",
       "      <td>Train_46456</td>\n",
       "      <td>165</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>ড়ি</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9359</th>\n",
       "      <td>Train_9359</td>\n",
       "      <td>33</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>ঙ্ক্তি</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19052</th>\n",
       "      <td>Train_19052</td>\n",
       "      <td>167</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>য়ে</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14356</th>\n",
       "      <td>Train_14356</td>\n",
       "      <td>71</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>থ্রি</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16131</th>\n",
       "      <td>Train_16131</td>\n",
       "      <td>138</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>শ্লু</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39623</th>\n",
       "      <td>Train_39623</td>\n",
       "      <td>52</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>ঞ্জে</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8155</th>\n",
       "      <td>Train_8155</td>\n",
       "      <td>125</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>ল্কি</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35747</th>\n",
       "      <td>Train_35747</td>\n",
       "      <td>96</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>প্রা</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38532</th>\n",
       "      <td>Train_38532</td>\n",
       "      <td>61</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ণ্ঠ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22574</th>\n",
       "      <td>Train_22574</td>\n",
       "      <td>107</td>\n",
       "      <td>4</td>\n",
       "      <td>4</td>\n",
       "      <td>ব্যু</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16186</th>\n",
       "      <td>Train_16186</td>\n",
       "      <td>56</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ড</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14931</th>\n",
       "      <td>Train_14931</td>\n",
       "      <td>65</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>ত্তে</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14025</th>\n",
       "      <td>Train_14025</td>\n",
       "      <td>129</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>ল্পী</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17636</th>\n",
       "      <td>Train_17636</td>\n",
       "      <td>25</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>গ্ন</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45846</th>\n",
       "      <td>Train_45846</td>\n",
       "      <td>149</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>স্ট</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26397</th>\n",
       "      <td>Train_26397</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>এঁ</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19783</th>\n",
       "      <td>Train_19783</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>ক্ট</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5021 rows × 5 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          image_id    ...     grapheme\n",
       "35486  Train_35486    ...         প্লা\n",
       "9103    Train_9103    ...            ঋ\n",
       "40498  Train_40498    ...          স্ট\n",
       "29403  Train_29403    ...         ষ্পে\n",
       "16191  Train_16191    ...           বে\n",
       "19359  Train_19359    ...           পা\n",
       "28480  Train_28480    ...           ধা\n",
       "34491  Train_34491    ...           ফে\n",
       "25116  Train_25116    ...       র্ধ্বে\n",
       "19475  Train_19475    ...         ম্লে\n",
       "40616  Train_40616    ...       ষ্ট্যে\n",
       "6096    Train_6096    ...           ভো\n",
       "12028  Train_12028    ...           তা\n",
       "6983    Train_6983    ...         ণ্ডা\n",
       "42458  Train_42458    ...         র্দা\n",
       "45570  Train_45570    ...          ন্স\n",
       "37821  Train_37821    ...            য\n",
       "17876  Train_17876    ...         ণ্ঠে\n",
       "32581  Train_32581    ...         প্তি\n",
       "31253  Train_31253    ...          হ্ম\n",
       "19141  Train_19141    ...           ড়ী\n",
       "1696    Train_1696    ...           পৈ\n",
       "7524    Train_7524    ...         ক্ষী\n",
       "14210  Train_14210    ...           ড়ু\n",
       "25742  Train_25742    ...         ল্টো\n",
       "43440  Train_43440    ...           হু\n",
       "50163  Train_50163    ...       স্থ্যে\n",
       "40888  Train_40888    ...            ক\n",
       "33397  Train_33397    ...         ন্মু\n",
       "3100    Train_3100    ...        র্ব্ব\n",
       "...            ...    ...          ...\n",
       "40980  Train_40980    ...           ভু\n",
       "43894  Train_43894    ...         ন্ডো\n",
       "31804  Train_31804    ...         র্ভু\n",
       "2148    Train_2148    ...         ল্কি\n",
       "12548  Train_12548    ...          ত্ন\n",
       "15114  Train_15114    ...           ঠে\n",
       "41939  Train_41939    ...         ক্তি\n",
       "4819    Train_4819    ...          র্ভ\n",
       "19458  Train_19458    ...           লী\n",
       "18449  Train_18449    ...       ত্ত্বী\n",
       "14737  Train_14737    ...         র্ধে\n",
       "44875  Train_44875    ...           খৈ\n",
       "43562  Train_43562    ...         দ্ভি\n",
       "46456  Train_46456    ...           ড়ি\n",
       "9359    Train_9359    ...       ঙ্ক্তি\n",
       "19052  Train_19052    ...           য়ে\n",
       "14356  Train_14356    ...         থ্রি\n",
       "16131  Train_16131    ...         শ্লু\n",
       "39623  Train_39623    ...         ঞ্জে\n",
       "8155    Train_8155    ...         ল্কি\n",
       "35747  Train_35747    ...         প্রা\n",
       "38532  Train_38532    ...          ণ্ঠ\n",
       "22574  Train_22574    ...         ব্যু\n",
       "16186  Train_16186    ...            ড\n",
       "14931  Train_14931    ...         ত্তে\n",
       "14025  Train_14025    ...         ল্পী\n",
       "17636  Train_17636    ...          গ্ন\n",
       "45846  Train_45846    ...          স্ট\n",
       "26397  Train_26397    ...           এঁ\n",
       "19783  Train_19783    ...          ক্ট\n",
       "\n",
       "[5021 rows x 5 columns]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
