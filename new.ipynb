{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import argparse\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.losses import categorical_crossentropy\n",
    "from tensorflow.keras.metrics import categorical_accuracy, top_k_categorical_accuracy\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau, TensorBoard\n",
    "\n",
    "from tensorflow.keras import layers as L\n",
    "import efficientnet.tfkeras as efn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(image):\n",
    "  # https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/main.py#L325-L326\n",
    "  # https://github.com/tensorflow/tpu/blob/master/models/official/efficientnet/efficientnet_builder.py#L31-L32\n",
    "  image -= tf.constant([0.485 * 255, 0.456 * 255, 0.406 * 255])  # RGB\n",
    "  image /= tf.constant([0.229 * 255, 0.224 * 255, 0.225 * 255])  # RGB\n",
    "  return image\n",
    "\n",
    "\n",
    "def get_model(input_size, backbone='efficientnet-b6', weights='imagenet', tta=False):\n",
    "  print(f'Using backbone {backbone} and weights {weights}')\n",
    "  x = L.Input(shape=input_size, name='imgs', dtype='float32')\n",
    "  y = normalize(x)\n",
    "  if backbone.startswith('efficientnet'):\n",
    "    model_fn = getattr(efn, f'EfficientNetB{backbone[-1]}')\n",
    "\n",
    "  y = model_fn(input_shape=input_size, weights=weights, include_top=False)(y)\n",
    "  y = L.GlobalAveragePooling2D()(y)\n",
    "  y = L.Dropout(0.2)(y)\n",
    "  # 1292 of 1295 are present\n",
    "  y = L.Dense(1292, activation='softmax')(y)\n",
    "  model = tf.keras.Model(x, y)\n",
    "\n",
    "  if tta:\n",
    "    assert False, 'This does not make sense yet'\n",
    "    x_flip = tf.reverse(x, [2])  # 'NHWC'\n",
    "    y_tta = tf.add(model(x), model(x_flip)) / 2.0\n",
    "    tta_model = tf.keras.Model(x, y_tta)\n",
    "    return model, tta_model\n",
    "\n",
    "  return model\n",
    "\n",
    "\n",
    "def mixup(img_batch, label_batch, batch_size):\n",
    "  # https://github.com/tensorpack/tensorpack/blob/master/examples/ResNet/cifar10-preact18-mixup.py\n",
    "  weight = tf.random.uniform([batch_size])\n",
    "  x_weight = tf.reshape(weight, [batch_size, 1, 1, 1])\n",
    "  y_weight = tf.reshape(weight, [batch_size, 1])\n",
    "  index = tf.random.shuffle(tf.range(batch_size, dtype=tf.int32))\n",
    "  x1, x2 = img_batch, tf.gather(img_batch, index)\n",
    "  img_batch = x1 * x_weight + x2 * (1. - x_weight)\n",
    "  y1, y2 = label_batch, tf.gather(label_batch, index)\n",
    "  label_batch = y1 * y_weight + y2 * (1. - y_weight)\n",
    "  return img_batch, label_batch\n",
    "\n",
    "\n",
    "def get_strategy():\n",
    "  # Detect hardware, return appropriate distribution strategy\n",
    "  try:\n",
    "    tpu = tf.distribute.cluster_resolver.TPUClusterResolver()  # TPU detection\n",
    "    print('Running on TPU ', tpu.cluster_spec().as_dict()['worker'])\n",
    "  except ValueError:\n",
    "    tpu = None\n",
    "\n",
    "  if tpu:\n",
    "    tf.config.experimental_connect_to_cluster(tpu)\n",
    "    tf.tpu.experimental.initialize_tpu_system(tpu)\n",
    "    strategy = tf.distribute.experimental.TPUStrategy(tpu)\n",
    "  else:\n",
    "    strategy = tf.distribute.get_strategy()\n",
    "\n",
    "  print('REPLICAS: ', strategy.num_replicas_in_sync)\n",
    "  return strategy\n",
    "\n",
    "\n",
    "def one_hot(image, label):\n",
    "  label = tf.one_hot(label, 1292)\n",
    "  return image, label\n",
    "\n",
    "\n",
    "def read_tfrecords(example, input_size):\n",
    "  features = {\n",
    "      'img': tf.io.FixedLenFeature([], tf.string),\n",
    "      'image_id': tf.io.FixedLenFeature([], tf.int64),\n",
    "      'grapheme_root': tf.io.FixedLenFeature([], tf.int64),\n",
    "      'vowel_diacritic': tf.io.FixedLenFeature([], tf.int64),\n",
    "      'consonant_diacritic': tf.io.FixedLenFeature([], tf.int64),\n",
    "      'unique_tuple': tf.io.FixedLenFeature([], tf.int64),\n",
    "  }\n",
    "  example = tf.io.parse_single_example(example, features)\n",
    "  img = tf.image.decode_image(example['img'])\n",
    "  img = tf.reshape(img, input_size + (1, ))\n",
    "  img = tf.cast(img, tf.float32)\n",
    "  # grayscale -> RGB\n",
    "  img = tf.repeat(img, 3, -1)\n",
    "\n",
    "  # image_id = tf.cast(example['image_id'], tf.int32)\n",
    "  # grapheme_root = tf.cast(example['grapheme_root'], tf.int32)\n",
    "  # vowel_diacritic = tf.cast(example['vowel_diacritic'], tf.int32)\n",
    "  # consonant_diacritic = tf.cast(example['consonant_diacritic'], tf.int32)\n",
    "  unique_tuple = tf.cast(example['unique_tuple'], tf.int32)\n",
    "  return img, unique_tuple\n",
    "\n",
    "\n",
    "def main():\n",
    "  parser = argparse.ArgumentParser()\n",
    "  parser.add_argument('--model_id', type=int, default=0)\n",
    "  parser.add_argument('--seed', type=int, default=123)\n",
    "  parser.add_argument('--lr', type=float, default=2e-4)\n",
    "  parser.add_argument('--input_size', type=str, default='160,256')\n",
    "  parser.add_argument('--batch_size', type=int, default=32)\n",
    "  parser.add_argument('--epochs', type=int, default=60)\n",
    "  parser.add_argument('--backbone', type=str, default='efficientnet-b5')\n",
    "  parser.add_argument('--weights', type=str, default='imagenet')\n",
    "  args, _ = parser.parse_known_args()\n",
    "\n",
    "  args.input_size = tuple(int(x) for x in args.input_size.split(','))\n",
    "  np.random.seed(args.seed)\n",
    "  tf.random.set_seed(args.seed)\n",
    "\n",
    "  # build the model\n",
    "  strategy = get_strategy()\n",
    "  with strategy.scope():\n",
    "    model = get_model(input_size=args.input_size + (3, ), backbone=args.backbone,\n",
    "        weights=args.weights)\n",
    "\n",
    "  model.compile(optimizer=Adam(lr=args.lr),\n",
    "                loss=categorical_crossentropy,\n",
    "                metrics=[categorical_accuracy, top_k_categorical_accuracy])\n",
    "  # print(model.summary())\n",
    "  AUTO = tf.data.experimental.AUTOTUNE\n",
    "  # create the training and validation datasets\n",
    "  train_fns = tf.io.gfile.glob('./records/train*.tfrec')\n",
    "  train_ds = tf.data.TFRecordDataset(train_fns, num_parallel_reads=AUTO)\n",
    "  train_ds = train_ds.map(lambda e: read_tfrecords(e, args.input_size), num_parallel_calls=AUTO)\n",
    "  train_ds = train_ds.repeat().batch(args.batch_size)\n",
    "  train_ds = train_ds.map(one_hot, num_parallel_calls=AUTO)\n",
    "  train_ds = train_ds.map(lambda a, b: mixup(a, b, args.batch_size), num_parallel_calls=AUTO)\n",
    "\n",
    "  val_fns = tf.io.gfile.glob('./records/val*.tfrec')\n",
    "  val_ds = tf.data.TFRecordDataset(val_fns, num_parallel_reads=AUTO)\n",
    "  val_ds = val_ds.map(lambda e: read_tfrecords(e, args.input_size), num_parallel_calls=AUTO)\n",
    "  val_ds = val_ds.batch(args.batch_size)\n",
    "  val_ds = val_ds.map(one_hot, num_parallel_calls=AUTO)\n",
    "\n",
    "  # train\n",
    "  num_train_samples = sum(int(fn.split('_')[2]) for fn in train_fns)\n",
    "  # num_val_samples = sum(int(fn.split('_')[2]) for fn in val_fns)\n",
    "  steps_per_epoch = num_train_samples // args.batch_size\n",
    "  print(f'Training on {num_train_samples} samples. Each epochs requires {steps_per_epoch} steps')\n",
    "  checkpoint = ModelCheckpoint(filepath=\"tmp-effb5-reborn-epoch{epoch:04}.h5\")\n",
    "  schedule = ReduceLROnPlateau(patience=5,verbose=1)\n",
    "  h = model.fit(train_ds, steps_per_epoch=steps_per_epoch, epochs=args.epochs, verbose=1,\n",
    "      validation_data=val_ds,callbacks=[checkpoint,schedule])\n",
    "  print(h)\n",
    "  model.save(\"B5-60-2.h5\")\n",
    "  return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "REPLICAS:  1\n",
      "Using backbone efficientnet-b5 and weights imagenet\n",
      "Training on 160672 samples. Each epochs requires 5021 steps\n",
      "Train for 5021 steps\n",
      "Epoch 1/60\n",
      "5021/5021 [==============================] - 2146s 427ms/step - loss: 3.9911 - categorical_accuracy: 0.4598 - top_k_categorical_accuracy: 0.6592 - val_loss: 0.3850 - val_categorical_accuracy: 0.9139 - val_top_k_categorical_accuracy: 0.9885\n",
      "Epoch 2/60\n",
      "5021/5021 [==============================] - 2121s 422ms/step - loss: 2.2247 - categorical_accuracy: 0.7392 - top_k_categorical_accuracy: 0.9126 - val_loss: 0.1936 - val_categorical_accuracy: 0.9507 - val_top_k_categorical_accuracy: 0.9949\n",
      "Epoch 3/60\n",
      "5021/5021 [==============================] - 2120s 422ms/step - loss: 1.7327 - categorical_accuracy: 0.7604 - top_k_categorical_accuracy: 0.9392 - val_loss: 0.1403 - val_categorical_accuracy: 0.9643 - val_top_k_categorical_accuracy: 0.9967\n",
      "Epoch 4/60\n",
      "5021/5021 [==============================] - 2057s 410ms/step - loss: 1.4984 - categorical_accuracy: 0.7690 - top_k_categorical_accuracy: 0.9548 - val_loss: 0.1457 - val_categorical_accuracy: 0.9655 - val_top_k_categorical_accuracy: 0.9968\n",
      "Epoch 5/60\n",
      "5021/5021 [==============================] - 2119s 422ms/step - loss: 1.3654 - categorical_accuracy: 0.7758 - top_k_categorical_accuracy: 0.9651 - val_loss: 0.1337 - val_categorical_accuracy: 0.9639 - val_top_k_categorical_accuracy: 0.9969\n",
      "Epoch 6/60\n",
      "5021/5021 [==============================] - 2119s 422ms/step - loss: 1.2731 - categorical_accuracy: 0.7812 - top_k_categorical_accuracy: 0.9711 - val_loss: 0.1211 - val_categorical_accuracy: 0.9673 - val_top_k_categorical_accuracy: 0.9975\n",
      "Epoch 7/60\n",
      "5021/5021 [==============================] - 2118s 422ms/step - loss: 1.2056 - categorical_accuracy: 0.7872 - top_k_categorical_accuracy: 0.9757 - val_loss: 0.1025 - val_categorical_accuracy: 0.9736 - val_top_k_categorical_accuracy: 0.9979\n",
      "Epoch 8/60\n",
      "5021/5021 [==============================] - 2124s 423ms/step - loss: 1.1547 - categorical_accuracy: 0.7893 - top_k_categorical_accuracy: 0.9789 - val_loss: 0.1157 - val_categorical_accuracy: 0.9691 - val_top_k_categorical_accuracy: 0.9973\n",
      "Epoch 9/60\n",
      "5021/5021 [==============================] - 2058s 410ms/step - loss: 1.1117 - categorical_accuracy: 0.7928 - top_k_categorical_accuracy: 0.9818 - val_loss: 0.1025 - val_categorical_accuracy: 0.9739 - val_top_k_categorical_accuracy: 0.9979\n",
      "Epoch 10/60\n",
      "5021/5021 [==============================] - 2117s 422ms/step - loss: 1.0724 - categorical_accuracy: 0.7952 - top_k_categorical_accuracy: 0.9838 - val_loss: 0.1024 - val_categorical_accuracy: 0.9734 - val_top_k_categorical_accuracy: 0.9978\n",
      "Epoch 11/60\n",
      "5021/5021 [==============================] - 2117s 422ms/step - loss: 1.0436 - categorical_accuracy: 0.7967 - top_k_categorical_accuracy: 0.9847 - val_loss: 0.0965 - val_categorical_accuracy: 0.9741 - val_top_k_categorical_accuracy: 0.9981\n",
      "Epoch 12/60\n",
      "5021/5021 [==============================] - 2116s 421ms/step - loss: 1.0166 - categorical_accuracy: 0.8002 - top_k_categorical_accuracy: 0.9862 - val_loss: 0.1035 - val_categorical_accuracy: 0.9716 - val_top_k_categorical_accuracy: 0.9980\n",
      "Epoch 13/60\n",
      "5021/5021 [==============================] - 2116s 421ms/step - loss: 0.9883 - categorical_accuracy: 0.8032 - top_k_categorical_accuracy: 0.9879 - val_loss: 0.1006 - val_categorical_accuracy: 0.9744 - val_top_k_categorical_accuracy: 0.9978\n",
      "Epoch 14/60\n",
      "5021/5021 [==============================] - 2055s 409ms/step - loss: 0.9663 - categorical_accuracy: 0.8045 - top_k_categorical_accuracy: 0.9891 - val_loss: 0.1004 - val_categorical_accuracy: 0.9754 - val_top_k_categorical_accuracy: 0.9976\n",
      "Epoch 15/60\n",
      "5021/5021 [==============================] - 2117s 422ms/step - loss: 0.9499 - categorical_accuracy: 0.8064 - top_k_categorical_accuracy: 0.9896 - val_loss: 0.1040 - val_categorical_accuracy: 0.9725 - val_top_k_categorical_accuracy: 0.9978\n",
      "Epoch 16/60\n",
      "5020/5021 [============================>.] - ETA: 0s - loss: 0.9303 - categorical_accuracy: 0.8082 - top_k_categorical_accuracy: 0.9901\n",
      "Epoch 00016: ReduceLROnPlateau reducing learning rate to 1.9999999494757503e-05.\n",
      "5021/5021 [==============================] - 2117s 422ms/step - loss: 0.9303 - categorical_accuracy: 0.8082 - top_k_categorical_accuracy: 0.9901 - val_loss: 0.0965 - val_categorical_accuracy: 0.9751 - val_top_k_categorical_accuracy: 0.9979\n",
      "Epoch 17/60\n",
      "5021/5021 [==============================] - 2117s 422ms/step - loss: 0.8442 - categorical_accuracy: 0.8210 - top_k_categorical_accuracy: 0.9938 - val_loss: 0.0773 - val_categorical_accuracy: 0.9797 - val_top_k_categorical_accuracy: 0.9986\n",
      "Epoch 18/60\n",
      "5021/5021 [==============================] - 2115s 421ms/step - loss: 0.8121 - categorical_accuracy: 0.8260 - top_k_categorical_accuracy: 0.9945 - val_loss: 0.0758 - val_categorical_accuracy: 0.9805 - val_top_k_categorical_accuracy: 0.9985\n",
      "Epoch 19/60\n",
      "5021/5021 [==============================] - 2115s 421ms/step - loss: 0.7947 - categorical_accuracy: 0.8289 - top_k_categorical_accuracy: 0.9952 - val_loss: 0.0735 - val_categorical_accuracy: 0.9813 - val_top_k_categorical_accuracy: 0.9986\n",
      "Epoch 20/60\n",
      "5021/5021 [==============================] - 2115s 421ms/step - loss: 0.7847 - categorical_accuracy: 0.8305 - top_k_categorical_accuracy: 0.9958 - val_loss: 0.0721 - val_categorical_accuracy: 0.9815 - val_top_k_categorical_accuracy: 0.9987\n",
      "Epoch 21/60\n",
      "5021/5021 [==============================] - 2122s 423ms/step - loss: 0.7789 - categorical_accuracy: 0.8318 - top_k_categorical_accuracy: 0.9956 - val_loss: 0.0711 - val_categorical_accuracy: 0.9818 - val_top_k_categorical_accuracy: 0.9987\n",
      "Epoch 22/60\n",
      "5021/5021 [==============================] - 2116s 422ms/step - loss: 0.7689 - categorical_accuracy: 0.8344 - top_k_categorical_accuracy: 0.9958 - val_loss: 0.0705 - val_categorical_accuracy: 0.9819 - val_top_k_categorical_accuracy: 0.9987\n",
      "Epoch 23/60\n",
      "5021/5021 [==============================] - 2118s 422ms/step - loss: 0.7645 - categorical_accuracy: 0.8358 - top_k_categorical_accuracy: 0.9961 - val_loss: 0.0704 - val_categorical_accuracy: 0.9817 - val_top_k_categorical_accuracy: 0.9985\n",
      "Epoch 24/60\n",
      "5021/5021 [==============================] - 2119s 422ms/step - loss: 0.7586 - categorical_accuracy: 0.8356 - top_k_categorical_accuracy: 0.9963 - val_loss: 0.0703 - val_categorical_accuracy: 0.9818 - val_top_k_categorical_accuracy: 0.9986\n",
      "Epoch 25/60\n",
      "5021/5021 [==============================] - 2118s 422ms/step - loss: 0.7531 - categorical_accuracy: 0.8370 - top_k_categorical_accuracy: 0.9961 - val_loss: 0.0708 - val_categorical_accuracy: 0.9818 - val_top_k_categorical_accuracy: 0.9986\n",
      "Epoch 26/60\n",
      "5021/5021 [==============================] - 2118s 422ms/step - loss: 0.7490 - categorical_accuracy: 0.8379 - top_k_categorical_accuracy: 0.9965 - val_loss: 0.0680 - val_categorical_accuracy: 0.9823 - val_top_k_categorical_accuracy: 0.9987\n",
      "Epoch 27/60\n",
      "5021/5021 [==============================] - 2118s 422ms/step - loss: 0.7448 - categorical_accuracy: 0.8401 - top_k_categorical_accuracy: 0.9964 - val_loss: 0.0689 - val_categorical_accuracy: 0.9823 - val_top_k_categorical_accuracy: 0.9986\n",
      "Epoch 28/60\n",
      "5021/5021 [==============================] - 2118s 422ms/step - loss: 0.7425 - categorical_accuracy: 0.8391 - top_k_categorical_accuracy: 0.9967 - val_loss: 0.0704 - val_categorical_accuracy: 0.9818 - val_top_k_categorical_accuracy: 0.9987\n",
      "Epoch 29/60\n",
      "5021/5021 [==============================] - 2119s 422ms/step - loss: 0.7373 - categorical_accuracy: 0.8414 - top_k_categorical_accuracy: 0.9970 - val_loss: 0.0683 - val_categorical_accuracy: 0.9821 - val_top_k_categorical_accuracy: 0.9987\n",
      "Epoch 30/60\n",
      "5021/5021 [==============================] - 2118s 422ms/step - loss: 0.7349 - categorical_accuracy: 0.8403 - top_k_categorical_accuracy: 0.9969 - val_loss: 0.0698 - val_categorical_accuracy: 0.9824 - val_top_k_categorical_accuracy: 0.9988\n",
      "Epoch 31/60\n",
      "3322/5021 [==================>...........] - ETA: 11:24 - loss: 0.7342 - categorical_accuracy: 0.8430 - top_k_categorical_accuracy: 0.9967"
     ]
    }
   ],
   "source": [
    "model=main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
